{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Estadística Bayesiana\n",
    "\n",
    "## Examen 1 - Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profesor\n",
    "\n",
    "### Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "1. Alvaro Montenegro, [Curso de Estadística Bayesiana](https://github.com/AprendizajeProfundo/Estadistica-Bayesiana), 2020\n",
    "2. De Paula Gilberto, [\"MODELOS DE REGRESSÃO com apoio computacional, ](https://www.ime.usp.br/~giapaula/texto_2013.pdf), Instituto de Matemática e Estatística  Universidade de São  Paulo\n",
    "3.  [\"Stan User's Guide. Version 2.22\", Stan Development Team, 2020.](https://mc-stan.org/users/documentation/)\n",
    "4. Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin, Osvaldo A., [ArviZ a unified library for exploratory analysis of Bayesian models in Python](https://arviz-devs.github.io/arviz/), [The Journal of Open Source Software], 2019.\n",
    "5. Richard McElreath, [\"Statistical Rethinking, A Bayesian Course with examples in R and Stan\"](http://xcelab.net/rmpubs/rethinking/Statistical_Rethinking_sample.pdf), version compilada en noviembre 9 de 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "   \n",
    "Bienvenidas y bienvenidos  al primer parcial. Recuerden que este parcial es para aprender principalmente y para consolidar sus conocimientos.\n",
    "\n",
    "Pueden trabajar en grupo, máximo dos personas. Preparen su informe en un cuaderno de Jupyter Lab. Sus códigos debe escribirlos en Pystan y Python.\n",
    "Suba la  solución de su examen antes de la media noche (12.00 de la noche) del día 03 de mayo de 2020 a las 11.59. Favor enviar el enlace de su drive, a las siguientes direcciones:  ammontenegrod@unal.edu.co, ammontenegrod@gmail.com, con permiso para para lectura.\n",
    "\n",
    "Puede modificar el archivo cuantas veces desee, luego de esa fecha. Sin embargo a partir de esa fecha el profesor queda en libertad de revisar y emitir una calificación preliminar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión de Conceptos (20%)\n",
    "\n",
    "Haga una breve disertación en sus propias palabras de lo estudiado hast ahora en el curso incluyendo los siguientes conceptos:\n",
    "\n",
    "1. Distribución de los parámetros.\n",
    "2. Distribución conjunta de parámetros y observaciones.\n",
    "3. Distribución posterior.\n",
    "4. Distribución predictiva.\n",
    "\n",
    "Puede apoyarse en la bibliografía propuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos Básicos de Estadística Bayesiana\n",
    "\n",
    "En la estadística Bayesiana se asume que tanto los parámetros (que llamaremos simplemente parámetro) como los datos son generados por distribuciones. Denotaremos por $\\mathbf{\\theta}$ al parámetro y por $\\mathbf{x}$ a las observaciones. Por facilidad asumiremos en este documento que $\\mathbf{\\theta}=(\\theta_1,\\ldots,\\theta_p)^t$ y que $\\mathbf{x}$ es una matrix aleatoria de tamaño $m\\times n $, en donde cada fila corresponde a una observación denotada por $\\mathbf{x}_i$. \n",
    "\n",
    "Adicionalmente asumiremos que las  $\\mathbf{x}_i$ son variables aleatorias condicionalmente independientes con respecto al parámetro, es decir $\\mathbf{x}_i|\\mathbf{\\theta}$ son idependientes. Si $f(\\mathbf{x}_i|\\mathbf{\\theta})$ denota la densidad de $\\mathbf{x}_i$ dado $\\mathbf{\\theta}$, entonces se tiene que\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}|\\mathbf{\\theta})= \\prod_{i=1}^n f(\\mathbf{x}_i|\\mathbf{\\theta}).\n",
    "$$\n",
    "\n",
    "Se observa entonces que $f(\\mathbf{x}|\\mathbf{\\theta})$ es una función de densidad condicional, la cual se denomina función de verosimilitud o simplemete verosimilitud. Es importante entender que esta definición diferencia a la estadística clásica o frecuentista de la estadística moderna o Bayesiana. En efecto, en la estadística clásica la verosimilitud es simplemente una función del parámetro y los datos (los cuales son conocidos). En la estadística Bayesiana, la verosimiltud es una función de densidad condicional. \n",
    "\n",
    "\n",
    "Estas son cantidades aleatorias que se suponen dependientes. El problema en la estadística Bayesiana es determinar cuál es el valor, digamos $\\mathbf{\\theta}^0$ de  $\\mathbf{\\theta}$ que es más plausible para los datos observados, los cuales denotaremos por $\\mathbf{x}^0$.\n",
    "\n",
    "Por simplicidad se puede pensar que  $\\mathbf{\\theta}$ es una variable latente, es decir,  una variable que no puede observarse directamente, mientras que las observaciones si son observables directactame.\n",
    "\n",
    "Dado que $\\mathbf{\\theta}$ y $\\mathbf{x}$ son objetos aleatorias (vectores y matrices) dependientes, se tiene que la distribución condicional de  $\\mathbf{\\theta}|\\mathbf{x}$ es dada de acuerdo con el teorema de Bayes por\n",
    "\n",
    "$$\n",
    "f(\\mathbf{\\theta}|\\mathbf{x}) = \\frac{f(\\mathbf{x}|\\mathbf{\\theta})p(\\mathbf{\\theta})}{\\int f(\\mathbf{x}|\\mathbf{\\theta})p(\\mathbf{\\theta}) d\\mathbf{\\theta}} = \\frac{f(\\mathbf{x}|\\mathbf{\\theta})p(\\mathbf{\\theta})}{p(\\mathbf{x})}.\n",
    "$$\n",
    "\n",
    "La función de densidad condicional $f(\\mathbf{\\theta}|\\mathbf{x})$ se denomina desnidad posterior o simplemente posterior. En esta expresión $p(\\mathbf{\\theta})$ y $p(\\mathbf{x})$ son las densidades marginales de $\\mathbf{\\theta}$ y $\\mathbf{x}$ respectivamente. La densidad $p(\\mathbf{\\theta})$ es en general desconocida y se denomina densidad a priori y su respectiva distribución se denominará distribución a priori. \n",
    "\n",
    "En el proceso de inferencia es necesario proponer distribuciones  apriori que tengan sentido para el parámetro  $\\mathbf{\\theta}^0$ que se desea estimar.  Por ejemplo, si  $\\mathbf{\\theta}^0$  corresponde a una probabilidad, es necesario proponer una distribución a priori con dominio (soporte) en $[0,1]$, como por ejemplo $U(0,1)$ o de manera más general $\\text{Beta}(a,b)$.\n",
    "\n",
    "\n",
    "Por otro lado, dada  $p(\\mathbf{\\theta})$ la densidad $p(\\mathbf{x})$ puede calcularse, dado que es una integral. No obstante este es justamente uno de los problemas de la estadística Bayesiana dado que la integral es por lo general intratable.\n",
    "\n",
    "Puede observarse que la densidad conjunta de $\\mathbf{\\theta}$ y $\\mathbf{x}$ puede escribirse como\n",
    "\n",
    "$$\n",
    "f(\\mathbf{\\theta}, \\mathbf{x}) = f(\\mathbf{\\theta}| \\mathbf{x})p(\\mathbf{x}) = f(\\mathbf{x}| \\mathbf{\\theta})p(\\mathbf{\\theta}).\n",
    "$$\n",
    "\n",
    "\n",
    "Para terminar este ensayo se introduce el concepto de densidad predictiva. Supóngase que $\\mathbf{y}$ es una observación no incluída en $\\mathbf{x}$. La densidad predictiva es la densidad que predice datos del modelo estadístico en estudio a partir de las observaciones. Técnicamente se define por $f(\\mathbf{y} |\\mathbf{x})$. Se puede entonces deducir que a partir de la independencia condicional  se tiene que \n",
    "$$\n",
    "f(\\mathbf{y} |\\mathbf{x}) = \\int f(\\mathbf{y}|\\mathbf{\\theta}) p(\\mathbf{\\theta}|\\mathbf{x})d\\mathbf{\\theta},\n",
    "$$\n",
    "\n",
    "\n",
    "Por lo que una regla para obtener una observación de $f(\\mathbf{y} |\\mathbf{x}=\\mathbf{x}_0)$ es obtener una muestra $\\mathbf{\\theta}^*$ de la posterior $p(\\mathbf{\\theta}|\\mathbf{x})$ y luego obtener la muestra requerida a partir de $ f(\\mathbf{y}|\\mathbf{\\theta}=\\mathbf{\\theta}^*)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Beta-Binomial 10%\n",
    "\n",
    "Considere un modelo de observación Binomial. $y\\sim\n",
    "Bin(n, \\pi)$. \n",
    "\n",
    "1. Considere la a priori plana $p(\\pi)\\varpropto 1$. encuentre\n",
    "la distribución a posteriori, y el estimador Bayesiano EAP de\n",
    "$\\pi$.\n",
    "2. Considere la a priori de Jeffreys $p(\\pi)\\varpropto\n",
    "\\left[I(\\pi)\\right]^{1/2}$. Encuentre la distribución a\n",
    "posteriori de $\\pi$ y el estimador Bayesiano EAP de $\\pi$.\n",
    "3. Considere la a priori general $Beta(\\alpha,\\beta)$. Encuentre la distribución a\n",
    "posteriori de $\\pi$, el estimador Bayesiano EAP de $\\pi$ y la varianza de la distribución posterior the  $\\pi$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo general\n",
    "\n",
    "Considere un modelo de observacional Binomial. $y \\sim Bin(n,\\pi)$.\n",
    "\n",
    "\n",
    "- La verosimilitud es dada por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(y|\\pi)= {n\\choose y} \\pi^{y}(1-\\pi)^{(n-y)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- A priori: el modelo con el cual conjuga y de donde se propone que proviene $\\pi$ es un modelo $Beta(\\alpha,\\beta)$ cuya función de densidad es dada por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\pi)=\\dfrac{1}{B(\\alpha,\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "  Donde $B(\\alpha,\\beta)$ es la función beta.\n",
    "\n",
    "- Posterior\n",
    "\n",
    "$$\n",
    "p(\\pi|y) \\propto \\pi^{y}(1-\\pi)^{(n-y)} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\propto \\pi^{y+\\alpha-1}(1-\\pi)^{n+y+\\beta-1},\n",
    "$$\n",
    "\n",
    "por lo que $p(\\pi|y) = \\text{Beta}(y+\\alpha, n+y+\\beta-1)$.\n",
    "\n",
    "\n",
    "\n",
    "### Esperanza posterior general\n",
    "\n",
    "- Para agilizar los cálculos se reparametriza definiendo $(\\mu, M)$ como $\\mu= \\alpha/(\\alpha+\\beta)$, siendo esta la media a priori y se define $M= \\alpha+\\beta$, como una medida de precisón de la apriori. Dado esto, la varianza a priori se define como $\\mu(1-\\mu)/(M+1)$ la cual resulta una función decreciente de $M$. La distribución marginal de $Y$ es llamada *beta-binomial*, y tiene la siguiente forma:\n",
    "$$\n",
    "\\begin{align}\n",
    "  E\\left[\\frac{Y}{n}\\right] &=\\mu\\\\\n",
    " Var\\left[\\frac{Y}{n}\\right] &= \\frac{\\mu(1-\\mu)}{n}\\left[ 1+ \\frac{n-1}{M+1} \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "- Con lo que finalmente la distribución posterior de $\\pi$ es de nuevo una distribución Beta de parámetros $Beta(Y + \\alpha, n − Y + \\beta)$, con:\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\hat{\\pi}=E\\left[\\pi|Y\\right] &= \\frac{M}{M+n}\\mu + \\frac{n}{M+n}\\left(\\frac{Y}{n}  \\right)\\\\\n",
    " Var\\left[\\pi|Y\\right] &= \\left[\\hat{\\pi}(1-\\hat{\\pi})/(M+n+1)  \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Este es el caso más general y de aquí se derivan los demás análisis en los literales propuestos.\n",
    "\n",
    "\n",
    "###  A priori plana $p(\\pi) \\propto 1$. \n",
    "\n",
    "\n",
    "- Como caso particular de la distribución Beta, cuando $\\alpha=1$ y $\\beta=1$ se vuelve una distribución Uniforme $(0,1)$, de esta manera al hacer los reemplazos llegamos al caso analizado, y así, $\\mu=1/2$ y $M=2$.\n",
    "\n",
    "  Así se hacen los reemplazos respectivos llegando a que la distribución a posteriori viene definida por $Beta(Y + 1, n − Y + 1)$, con el estimador EAP dado por:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\hat{\\pi}=E\\left[\\pi|Y\\right] &= \\frac{M}{M+n}\\mu + \\frac{n}{M+n}\\left(\\frac{Y}{n}  \\right)\\\\\n",
    "           = \\frac{Y+1}{n+2}\n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### A priori de Jeffreys $p(\\pi) \\propto[I(\\pi)]^{1/2}$. E\n",
    "\n",
    "Para obtener la a priori de Jeffreys $p_J(\\theta)$ se desagrega la función de verosimilitud de la distribución binomial y se calcula su segunda derivada sobre el parámetro de interés $\\theta$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "l := \\log(p(y | \\theta)) &\\propto y \\log(\\theta) + (n-y) \\log(1-\\theta) \\\\\n",
    "\\frac{\\partial l }{\\partial \\theta} &= \\frac{y}{\\theta} - \\frac{n-y}{1-\\theta} \\\\\n",
    "\\frac{\\partial^{2} l }{\\partial \\theta^{2}} &= -\\frac{y}{\\theta^{2}} - \\frac{n-y}{ (1-\\theta)^{2} }\n",
    "\\end{align*}$$\n",
    "\n",
    "Así, la información de Fisher es:\n",
    "\n",
    "$$\\begin{align*}\n",
    "I(\\theta) &= -E(\\frac{\\partial^{2} l }{\\partial \\theta^{2}} | \\theta) \\\\\n",
    "&= \\frac{n\\theta}{\\theta^{2}} + \\frac{n - n \\theta}{(1-\\theta)^{2}} \\\\\n",
    "&= \\frac{n}{\\theta ( 1- \\theta)} \\\\\n",
    "&\\propto \\theta^{-1} (1-\\theta)^{-1}.\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Y la a priori de Jeffreys queda como:\n",
    "\n",
    "$$\\begin{align*}\n",
    "I(\\theta) &= -E(\\frac{\\partial^{2} l }{\\partial \\theta^{2}} | \\theta) \\\\\n",
    "&= \\frac{n\\theta}{\\theta^{2}} + \\frac{n - n \\theta}{(1-\\theta)^{2}} \\\\\n",
    "&= \\frac{n}{\\theta ( 1- \\theta)} \\\\\n",
    "&\\propto \\theta^{-1} (1-\\theta)^{-1}.\n",
    "\\end{align*}$$\n",
    "\n",
    "Siendo esta una distribución $Beta(1/2,1/2)$. Utilizando el mismo argumento del literal anterior, se hacen los reemplazos respectivos, con lo que la distribución a posterior es una $Beta(Y + 1/2, n − Y + 1/2)$, con el estimador EAP dado por:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\hat{\\pi}=E\\left[\\pi|Y\\right] &= \\frac{M}{M+n}\\mu + \\frac{n}{M+n}\\left(\\frac{Y}{n}  \\right)\\\\\n",
    "           = \\frac{Y+2}{n+1}\n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### A priori general $Beta(\\alpha; \\beta)$. \n",
    "\n",
    "Encuentre la distribución a posteriori de $\\pi$, el estimador bayesiano EAP de $\\pi$ y la varianza de la distribución posterior de $\\pi$.\n",
    "\n",
    "- Sea el modelo observacional:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(y|\\pi)= {n\\choose y} \\pi^{y}(1-\\pi)^{(n-y)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- El modelo con el cual conjuga y de donde proviene $\\pi$ se define como un modelo $Beta(\\alpha,\\beta)$ así:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\pi)=\\dfrac{1}{B(\\alpha,\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "  Donde $B(\\alpha,\\beta)$ es la función beta.\n",
    "\n",
    "- Para agilizar los cálculos se reparametriza definiendo $(\\mu, M)$ como $\\mu= \\alpha/(\\alpha+\\beta)$, siendo esta la media a priori y se define $M= \\alpha+\\beta$, como una medida de precisón de la apriori. Dado esto, la varianza a priori se define como $\\mu(1-\\mu)/(M+1)$ la cual resulta una función decreciente de $M$. La distribución marginal de $Y$ es llamada *beta-binomial*, y tiene la siguiente forma:\n",
    "$$\n",
    "\\begin{align}\n",
    "  E\\left[\\frac{Y}{n}\\right] &=\\mu\\\\\n",
    " Var\\left[\\frac{Y}{n}\\right] &= \\frac{\\mu(1-\\mu)}{n}\\left[ 1+ \\frac{n-1}{M+1} \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "- Con lo que finalmente la distribución posterior de $\\pi$ es de nuevo una distribución Beta de parámetros $Beta(Y + \\alpha, n − Y + \\beta)$, con:\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\hat{\\pi}=E\\left[\\pi|Y\\right] &= \\frac{M}{M+n}\\mu + \\frac{n}{M+n}\\left(\\frac{Y}{n}  \\right)\\\\\n",
    " Var\\left[\\pi|Y\\right] &= \\left[\\hat{\\pi}(1-\\hat{\\pi})/(M+n+1)  \\right].\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo para datos asimétricos 35%\n",
    "\n",
    "Los datos de la tabla 1 muestran los resultados de un experimento conducido para evaluar el desempeño de cinco tipos de turbina de alta velocidad para motores de aviación. Fueron considerados 10 motores de cada tipo para el análisis. Verifique que es razonable suponer que cada  medida $T_{ij}, i =1,\\ldots,5, j=1,\\ldots,10$  puede ser modelada como  $T_{ij} \\sim Gamma(\\mu_i,\\phi)$, en donde \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mu_i= \\mu + \\beta_i, i =1,\\dots,5\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "|Tipo 1 | Tipo 2 | Tipo 3 | Tipo 4 | Tipo 5|\n",
    "|---|---|---|---|---|\n",
    "|3.03    |   3.19    |    3.46   |    5.88     |     6.43|\n",
    "|5.53    |    4.26   |   5.22    |     6.74    |     9.97|\n",
    "|5.60    |    4.47   |    5.69   |     6.90    |    10.39|\n",
    "|9.30    |    4.53   |    6.54   |     6.98    |    13.55|\n",
    "|9.92    |    4.67   |    9.16   |     7.21    |    14.45|\n",
    "|12.51   |    4.69   |    9.40   |     8.14    |    14.72|\n",
    "|15.21   |    6.79   |   10.71   |     9.80    |    18.39|\n",
    "|16.84   |  12.75    |   13.41   |    25.46    |    21.51|\n",
    "\n",
    "*Tabla 1. Tiempo (en millones de ciclos) hasta perder velocidad por tipo de turbina, de 10 10 motores por cada tipo de turbina*\n",
    "\n",
    "Use Stan para obtener una estimación de los parámetros $\\mu, \\beta_i, i=1,\\ldots,5$, y $\\phi$. Haga una discusión detallada de los resultados. Incluya los datos, gráficas y estadísticas que considere necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pystan\n",
    "import random\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = open('./datos/turbina.dat')\n",
    "datos = np.loadtxt('./datos/turbina.dat')\n",
    "datos = pd.DataFrame({\"Tiempo\":datos[:,np.arange(1,10,2)].reshape(5*10),\n",
    "         \"Tipo\":datos[:,np.arange(0,9,2)].reshape(5*10)})\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =  plt.figure(figsize=(10,7))\n",
    "sns.boxplot(y=datos[\"Tiempo\"],x=datos[\"Tipo\"] )\n",
    "fig.suptitle('Gráficos boxplot por tipo de moror',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que el modelos para nuestros datos esta dado por:\n",
    "\n",
    "$$T_{ij} \\sim Gamma(\\mu_i,\\phi) \\quad ln(\\mu_i) = \\mu + \\beta_i \\quad i = 1,\\cdots,5$$\n",
    "\n",
    "Donde este corresponde a el modelo Gamma reparametrizado de la forma:\n",
    "$$\n",
    "\\begin{align}\n",
    "a &= \\mu_i^2/\\phi \\\\\n",
    "b &= \\mu/\\phi_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$f(y_{ij};\\mu_i,\\phi) = \\frac{(\\mu_i/\\phi)^{\\mu_i^2/\\phi}} {\\Gamma(\\mu_i^2/\\phi)} \\, y^{\\mu_i^2/\\phi - 1} \\exp(-\\mu_i/\\phi \\, y)$$\n",
    "\n",
    "A continuación procedemos a definir el modelo en STAN para que los procedimientos respectivos para obtener nuestros parámetros de interés ($\\beta_i$ y $\\phi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "N = len(datos)\n",
    "J = 5\n",
    "X = np.array(pd.get_dummies(datos['Tipo']))\n",
    "y = np.array(datos[\"Tiempo\"])\n",
    "scale_beta = 5\n",
    "data = OrderedDict({'N':N,'J':J,'scale_beta': scale_beta,'X':X, 'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic STAN\n",
    "import pystan\n",
    "%load_ext stanmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stan -f gamma.stan\n",
    "//  Gamma Regression  Model\n",
    "data {\n",
    "  int<lower=1> N;\n",
    "  int<lower=1> J;\n",
    "  real<lower=0.0> scale_beta; //parametro de esacala para priory de beta\n",
    "  vector<lower=0>[N] y; //datos\n",
    "  matrix[N,J] X; //matrix de dise?o para el modelo\n",
    "}\n",
    "parameters {\n",
    "  real b;\n",
    "  vector[J] beta;\n",
    "  real<lower=0> phi;\n",
    "}\n",
    "transformed parameters {\n",
    "  vector<lower=0>[N] mu;\n",
    "  for (i in 1:N) {\n",
    "    mu[i]  = exp(b + X[i,] * beta);\n",
    "  }\n",
    "}\n",
    "model {\n",
    "  // priors\n",
    "  b ~ normal(0.0,0.1);\n",
    "  beta ~ normal(0.0,0.1);\n",
    "  // likelihood\n",
    "  for (i in 1:N) {\n",
    "  y[i] ~ gamma(pow(mu[i],2)/phi,mu[i]/phi);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = pystan.StanModel(file='gamma.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(127)\n",
    "fit1 = modelo.sampling(data=data, iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit1.stansummary([\"b\",\"phi\",\"beta\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las salidas anteriores podemos observar las correspondientes estimaciones para nuestros parámetros $\\beta_i$, $\\mu$ y $\\phi$ correspondientes a el modelo de regresión gamma definido en la primera parte. Ademas en estas salidas obtenemos algunas medidas para diagnosticar que el algoritmo nos dio resultados adecuados, como el R-hat (nos cuantifica la consistencia de conjunto de cadenas de Markov) que en este caso dio para todos 1 luego nuestras cadenas se han mezclado bien.\n",
    "\n",
    "Ahora, respecto a las estimaciones como primera observación tenemos que todos los parámetros están contenidos en su intervalo de confianza 95\\% (entre los cuantiles 25\\%  y 75\\%), respecto a las lecturas tenemos que la media del efecto global esta dado por $exp(0.56) = 1.750673$ millones de ciclos hasta perder velocidad, que se aumenta mas para el \"Tipo de motor 2\" $exp(0.56 + 0.14) = 2.013753$.\n",
    "\n",
    "#### Análisis de los gráficos\n",
    "A continuación, se evalúa para el modelo anterior si las cadenas en nuestro proceso de muestreo convergen (grafico para las cadenas) y son no correlacionadas (grafico de autocorrelaciones). Esto para asegurarnos de que podemos confiar en las estimaciones obtenidas (Teorema ergódico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficos diagnostico para efecto global (b)\n",
    "cadenas = fit1.extract(permuted=False)\n",
    "cadena_i = cadenas[:,:,0]\n",
    "fig =  plt.figure(figsize=(15,7))\n",
    "grid = plt.GridSpec(2,2,hspace=0.15,wspace=0.15)\n",
    "ax11 = fig.add_subplot(grid[0,0])\n",
    "ax12 = fig.add_subplot(grid[0,1])\n",
    "ax21 = fig.add_subplot(grid[1,:])\n",
    "pd.DataFrame(data=cadena_i[0:100,:],columns = np.arange(1,5)).plot(ax = ax21)\n",
    "pd.DataFrame(data=cadena_i,columns = np.arange(1,5)).plot.kde(ax = ax12)\n",
    "#plot_acf(cadena_i.reshape((1,500*4)), lags=10, ax = ax11)\n",
    "fig.suptitle('Gráficos diagnostico para cadenas $b$',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficos diagnostico para efecto de phi\n",
    "cadena_i = cadenas[:,:,1]\n",
    "fig =  plt.figure(figsize=(15,7))\n",
    "grid = plt.GridSpec(2,2,hspace=0.15,wspace=0.15)\n",
    "ax11 = fig.add_subplot(grid[0,0])\n",
    "ax12 = fig.add_subplot(grid[0,1])\n",
    "ax21 = fig.add_subplot(grid[1,:])\n",
    "pd.DataFrame(data=cadena_i[0:100,:],columns = np.arange(1,5)).plot(ax = ax21)\n",
    "pd.DataFrame(data=cadena_i,columns = np.arange(1,5)).plot.kde(ax = ax12)\n",
    "#plot_acf(cadena_i.reshape((1,500*4)), lags=10, ax = ax11)\n",
    "fig.suptitle('Gráficos diagnostico para cadenas $\\mu$',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficos diagnostico para efecto motor j (beta_j)\n",
    "cadenas = fit1.extract(permuted=False)\n",
    "for j in range(5):\n",
    "    cadena_i = cadenas[:,:,j + 2]\n",
    "    fig =  plt.figure(figsize=(15,7))\n",
    "    grid = plt.GridSpec(2,2,hspace=0.15,wspace=0.15)\n",
    "    ax11 = fig.add_subplot(grid[0,0])\n",
    "    ax12 = fig.add_subplot(grid[0,1])\n",
    "    ax21 = fig.add_subplot(grid[1,:])\n",
    "    pd.DataFrame(data=cadena_i[0:100,:],columns = np.arange(1,5)).plot(ax = ax21)\n",
    "    pd.DataFrame(data=cadena_i,columns = np.arange(1,5)).plot.kde(ax = ax12)\n",
    "    #plot_acf(cadena_i.reshape((1,500*4)), lags=10, ax = ax11)\n",
    "    fig.suptitle('Gráficos diagnostico para cadenas $\\\\beta_{:3n}$'.format(j + 1),fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo para datos binarios 35%\n",
    "\n",
    "La tabla 2 muestra  los datos experimentales sobre la influencia de la razón y del volúmen del aire aspirado en la constricción vascular de la piel de los dedos de la mano, de un experimento reportado por Predigon. \n",
    "\n",
    "\n",
    "\n",
    "La respuesta es dicotómica: ocurrencia (1), y no ocurrecia (0). Se asume para cada observación $Y_i \\sim Ber(\\pi_i)$. El modelo lineal logistico es definido por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\log \\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_1 + \\beta_2 \\log(\\text{volúmen})_i + \\beta_3 \\log(\\text{razón})_i.\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Use Stan para obtener una estimación de los parámetros $\\beta_i, i=1,\\ldots3$. Haga una discusión detallada de los resultados. Incluya los datos, gráficas y estadísticas que considere necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos2 = np.loadtxt('./datos/pregibon.dat')\n",
    "datos2 = pd.DataFrame({\"Volumen\":datos2[:,1],\"Razon\":datos2[:,2],\"Respuesta\":datos2[:,0]})\n",
    "datos2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =  plt.figure(figsize=(10,7))\n",
    "sns.lmplot( x=\"Volumen\", y=\"Razon\", data=datos2, fit_reg=False, hue='Respuesta', legend=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta es dicotomica: ocurrencia (1), y no ocurrecia (0). Se asume para cada observacion $Y_i \\sim Ber(\\pi_i)$. El modelo lineal logistico es definido por:\n",
    "\n",
    "$$logit(\\pi_i) = log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_1 + \\beta_2 log(Vol_i) + \\beta_3 log(Raz_i)$$\n",
    "\n",
    "Dejamos ademas distribuciones priory para: $\\beta_i \\sim N(0,3)$. A continuacion procedemos a definir el modelo en STAN para que los porcedimientos respectivos para obtener muestros parametros de interes ($\\beta_i$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N2 = len(datos2)\n",
    "J2 = 2\n",
    "X2 = np.array(datos2[{\"Volumen\",\"Razon\"}])\n",
    "y2 = np.int_(np.array(datos2[\"Respuesta\"]))\n",
    "scale_beta2 = 5\n",
    "data2 = OrderedDict({'N':N2,'J':J2,'scale_beta': scale_beta2,'X':X2, 'y':y2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic STAN\n",
    "import pystan\n",
    "%load_ext stanmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stan -f logmodel.stan\n",
    "//Modelo lineal logistico\n",
    "data {\n",
    "  int<lower=1> N;\n",
    "  int<lower=1> J;\n",
    "  real<lower=0.0> scale_beta;\n",
    "  int<lower=0,upper=1> y[N];\n",
    "  matrix[N,J] X;\n",
    "}\n",
    "parameters {\n",
    "  vector[J] beta;\n",
    "  real alpha;\n",
    "}\n",
    "transformed parameters {\n",
    "  vector<lower=0>[N] pii;\n",
    "  for (i in 1:N) {\n",
    "    pii[i]  = inv_logit(alpha + log(X[i,])*beta);\n",
    "  }\n",
    "}\n",
    "model {\n",
    "  // priors\n",
    "  alpha ~ normal(0.0, scale_beta);\n",
    "  beta ~ normal(0.0, scale_beta);\n",
    "  // likelihood\n",
    "  for (i in 1:N) {\n",
    "  y[i] ~ bernoulli(pii[i]);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo2 = pystan.StanModel(file='logmodel.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "fit2 = modelo2.sampling(data=data2, iter= 2000, chains=4,warmup = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo que en el punto anterior, en la salida podemos observar las correspondientes estimaciones para nuestros parámetros $\\beta_i$, correspondientes a el modelo lineal logístico que fue definido al principio. Del mismo modo para diagnosticar que el algoritmo nos dio resultados adecuados, el R-hat resulto 1 en todos los casos luego nuestras cadenas se han mezclado bien.\n",
    "\n",
    "Ahora, respecto a las estimaciones, como primera observación tenemos que todos los parámetros están contenidos en su intervalo de confianza 95\\% , respecto a las lecturas tenemos que el riesgo base para que se presente constriccion vascular de la piel de los dedos de la mano es de $exp(-2.56) = 0.07502$, y ambos factores Volumen y Razon de aire asperado está provocando un aumento en el riesgo. En el caso de Volumen un aumento en una unidad (manteniendo razon constante) aumenta el riesgo en $exp(4.16) = 68.03348$, y para caso de la Razon este factor es de $exp(4.88) = 137.002613$.\n",
    "\n",
    "#### Análisis de los gráficos\n",
    "\n",
    "A continuación, se evalúa para el modelo anterior si las cadenas en nuestro proceso de muestreo convergen (grafico para las cadenas) y son no correlacionadas (grafico de autocorrelaciones). Esto para asegurarnos de que podemos confiar en las estimaciones obtenidas (Teorema ergódico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = fit2.summary()\n",
    "df = pd.DataFrame(summary_dict['summary'], \n",
    "                  columns=summary_dict['summary_colnames'], \n",
    "                  index=summary_dict['summary_rownames'])\n",
    "beta1 = df[\"mean\"][\"alpha\"]\n",
    "beta2 = df[\"mean\"][\"beta[1]\"]\n",
    "beta3 = df[\"mean\"][\"beta[2]\"]\n",
    "\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show\n",
    "\n",
    "\n",
    "def z_func(x,y):\n",
    " return np.exp(beta1 + beta2*np.log(x) + beta3*np.log(y))\n",
    " \n",
    "x = np.arange(min(datos2[\"Volumen\"]),max(datos2[\"Volumen\"]),0.1)\n",
    "y = np.arange(min(datos2[\"Razon\"]),max(datos2[\"Razon\"]),0.1)\n",
    "X,Y = meshgrid(x,y)\n",
    "Z = z_func(X, Y)\n",
    "\n",
    "fig =  plt.figure(figsize=(15,7))\n",
    "im = imshow(Z,cmap=cm.RdBu)\n",
    "cset = contour(Z,np.arange(0.25,0.76,0.5),linewidths=2,cmap=cm.Set1)\n",
    "clabel(cset,inline=True,fmt='%1.1f',fontsize=10)\n",
    "colorbar(im)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
