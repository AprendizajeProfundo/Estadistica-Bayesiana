{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación y selección de modelos Bayesianos de regresión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Montenegro, [Curso de Estadística Bayesiana](https://github.com/AprendizajeProfundo/Estadistica-Bayesiana), 2020\n",
    "2. Guangyuan Gao, \"Bayesian Claims Reserving Methods in Non-life Insurance with Stan. An Introduction\", Springer, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "En esta lección, revisamos las herramientas de diagnóstico del modelo, incluidas las predictivas posteriores control y gráficos residuales. \n",
    "\n",
    "También discutimos los criterios de selección del modelo, incluyendo\n",
    "Varios criterios de información y validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chequeo de la predictiva posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "En el marco Bayesiano, idealmente queremos dividir los datos en un conjunto de entrenamiento y un conjunto de prueba y hacer la verificación predictiva posterior en el conjunto de datos de prueba.\n",
    "\n",
    "\n",
    "Alternativamente, podemos elegir una estadística de prueba cuya distribución predictiva no dependa de parámetros desconocidos en el modelo, sino principalmente de la hipótesis que se verifica.\n",
    "\n",
    "\n",
    "Entonces no hay necesidad de tener un conjunto de datos de prueba por separado. Sin embargo, cuando se usan los mismos datos para ajustar y verificar el modelo, esto debe llevarse a cabo con precaución, ya que el\n",
    "el procedimiento puede ser conservador.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribución predictiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La distribución predictiva es la distribución de observaciones futuras dada la muestra actual.\n",
    "\n",
    "Supongamos que $ y_ {n + 1} $ es una observación futura que es independiente de los datos observados $ \\mathbf{y}$, dado el correspondiente parámetro $\\boldsymbol{\\theta}$. \n",
    "\n",
    "\n",
    "Entonces, la distribución predictiva para $ y_{n + 1} $ viene dada por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(y_{n+1}|\\mathbf{y}).\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Observe que\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y_{n+1}|\\mathbf{y})&=\\int\n",
    "p(y_{n+1},\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}\\\\\n",
    "&= \\int\n",
    "f(y_{n+1}|\\mathbf{\\theta},\\mathbf{y})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}\\\\\n",
    "&=\\int\n",
    "f(y_{n+1}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Es decir que\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(y_{n+1}|\\mathbf{y}) = \\int\n",
    "f(y_{n+1}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "y en consecuencia los valores predichos pueden ser calculados dentro del algorimo de muestreo.  Supongamos que en el paso $m$ después de convergencia se tiene que la muestra para $\\boldsymbol{\\theta}$ es $\\boldsymbol{\\theta}^{(m)}$. Entonces un valor predictivo puede ser obtenido a partir de la densidad posterior\n",
    "predictiva\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(y|\\mathbf{\\theta}^{(m)})\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Dentro del algoritmo de muestreo se procede así: en el paso $m$\n",
    "\n",
    "1. Se obtiene la muestra $\\mathbf{\\theta}^{(m)} \\sim p(\\mathbf{\\theta}|\\mathbf{y})$ (sección *model* de Stan).\n",
    "2. Se obtiene el valor predicho $y_{n+1} \\sim f(y|\\mathbf{\\theta}^{(m)})$ (sección *generated quantities* de Stan).\n",
    "\n",
    "Observe que en el numeral 2, se está usando la función de verosimilitud (que es una densidad) con parámetro $\\mathbf{\\theta}^{(m)}$.\n",
    "\n",
    "En el caso de los modelos de regresión cada observación $y_i$ depende no solamente del parámetro $\\mathbf{\\theta}$, sino también de un vector de variables regresoras o predictoras $\\mathbf{x}_i$. En este caso tenemos que la posterior predictiva es dada por \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(y_{n+1}|\\mathbf{y},x_{n+1}) = \\int\n",
    "f(y_{n+1}|\\mathbf{\\theta},x_{n+1})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo Modelo de Regresión Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección [Modelos Bayesianos de regresión](./Modelos_Bayesianos_Regresion.ipynb) introdujimos el modelo de regresión Binomial dado por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[y_i] &= \\mu_i= N_i\\pi_i\\\\\n",
    "\\pi_i &=  \\text{inv_logit}(\\alpha +  \\mathbf{x_i}^T\\mathbf{\\beta}) \\\\\n",
    "\\alpha &\\sim \\mathcal{N}(\\mu_{\\alpha},\\sigma_{\\alpha}^2)\\\\\n",
    "\\mathbf{\\beta} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\beta},\\rho_{\\beta}^2\\mathbf{I})\\\\\n",
    "y_i &\\sim \\text{Binomial}(N_i,\\pi_i), \\hspace{3mm} i =1,\\ldots,n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aqui cada observación $y_i$ depende de $\\pi_i$ y de su vector predictor $\\mathbf{x}_i$. Entonces si $\\alpha^{(m)}$ y $\\mathbf{\\beta}^{(m)}$ son las muestras respectivamwnte de $\\alpha$ y $\\mathbf{\\beta}$  en el paso $m$ del algortimo de muestreo. Entonces una muestra $y_*$ (no observada) en este paso dado el vector predictor $\\mathbf{x}_*$, el cual no hace parte de los datos se obtiene como sigue. Sea $N_*$ el número de ensayos, y denotemos por $y_*^{(m)}$ la predicción de $y_*$  en el paso $m$. Entonces se tiene  que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\pi}_*^{(m)} &=  \\text{inv_logit}(\\alpha^{(m)} +  \\mathbf{x}_*^T{\\mathbf{\\beta}^{(m)}}) \\\\\n",
    "y_*^{(m)} & \\sim \\text{Binomial}(N_*,{\\pi}_*^{(m)}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "En Stan $\\alpha$ y $\\mathbf{\\beta}$  son obtenidos en la sección *model* y los dos pasos anteriores para obtener la predicción son escritos en la sección *generated quantities*.\n",
    "\n",
    "El siguiente fragmento de código ilustra como implementar el algoritmo anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic STAN\n",
    "import pystan\n",
    "%load_ext stanmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stan -f binomial_reg.stan\n",
    "//  Binomial Regression  Model\n",
    "\n",
    "data{\n",
    "int<lower=0> n; // número de datos\n",
    "int <lower=0> p; // número de variables predictoras\n",
    "real<lower=0>[n] N; // número de ensayos para cada observación\n",
    "int<lower=0> y; // datos observados\n",
    "matrix[N,p] X; // variables predictoras observadas\n",
    "real[n] x_p; // vector predictor np observado\n",
    "int<lower=0> Np; // Número de ensayos para la predicción\n",
    "}\n",
    "\n",
    "parameters{\n",
    "    real alpha;\n",
    "    real[p] beta;   \n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  real{lower=0, upper=1} pi; \n",
    "  pi = inv_logit(alpha + X * beta);\n",
    "}\n",
    "\n",
    "\n",
    "model{\n",
    "  // priors\n",
    "  alpha ~ normal(0,10);\n",
    "  beta ~ normal(0,10);\n",
    "  // likelihood   \n",
    "  y ~ binomial_logit(N, pi)\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    real y_pred;\n",
    "    real p;\n",
    "    p = inv_logit(alpha + dot_product(x_p,beta))\n",
    "    y_pred = binomial_rng(Np,p)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<h2> 27.4 Residuals, Deviance and Deviance Residuals</h2>\n",
    "\n",
    "In the Bayesian framework, we can generate a set of residuals for one realization of\n",
    "posterior parameters. So there are four choices of residuals:\n",
    "1. Choose the posterior mean of parameters and find one set of residuals.\n",
    "2. Randomly choose a realization of parameters and find one set of the residuals.\n",
    "3. Get the posterior mean of residuals.\n",
    "4. Get the posterior distribution of residuals.\n",
    "\n",
    "In the following, we will review Pearson residuals, deviance and deviance residuals.\n",
    "A Pearson residual is defined as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "r_i(\\theta)= \\frac{ y_i -\n",
    "\\mathbb{E}(Y_i|\\theta)}{\\sqrt{Var(Y_i|\\theta)}}, \\quad\n",
    "i=1,\\cdots,n\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The **deviance** is defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "D(\\theta) := −2 \\log p(y|\\theta) = −2\\sum_{i=1}^n \\log p (yi |\\theta) ,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and the contribution of each data point to the deviance is $D_i(\\theta) = −2 \\log p (yi |\\theta)$. We will define and use $D(\\hat{\\theta})$ and $\\widehat{D(\\theta)}$ in the next section.\n",
    "\n",
    "The deviance residuals are based on a standardized or “saturated” version of the\n",
    "deviance, defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "D_S(\\theta) := −2 \\sum_{i=1}^n \\log p (y_i |\\theta) + 2 \\sum_{i=1}^n \\log p (yi |\\hat{\\theta}_S(y_i),\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\theta}_S(y_i)$ are appropriate “saturated” estimates, e.g., we set $\\hat{\\theta}_S(y)=y$. \n",
    "\n",
    "The contribution of each data point to the standardized deviance is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "D_{S_i}(\\theta) := −2  \\log p (y_i |\\theta) + 2  \\log p (yi |\\hat{\\theta}_S(y_i),\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The deviance residual is defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "dr_i := sign_i \\sqrt{D_{S_i}(\\theta)},\n",
    "\\end{equation}\n",
    "$$\n",
    "where $sign_i$ is the sign of $y_i − \\mathbb{E}(y_i|\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 27.5 Three error structures for stack-loss data</h2>\n",
    "\n",
    "Birkes and Dodge (1993) apply different regression models to the much-analysed stack-loss data of Brownlee (1965). This features 21 daily responses of stack loss $y$, the amount of ammonia escaping, with covariates being air flow ($x_1$) , temperature ($x_2$) and acid concentration ($x_3$). \n",
    "\n",
    "We first assume a linear regression on the expectation of $y$, with a variety of different error structures. We assume a linear regression on the expectation of y, i.e.,\n",
    "\n",
    "$$ \\mathbb{E}(y_i) = \\mu_i = b_0 + b_1 z_{1i} + b_2 z_{2i} + b_3 z_{3i} $$\n",
    "\n",
    "\n",
    "   \n",
    "where $z_{ij} = (x_{ij} - \\bar{x}_j ) / \\text{sd}(x_j )$ are covariates standardised to have zero mean and unit variance. $b_1 , b_2 , b_3$ are initially given independent \"noninformative\" priors. \n",
    "\n",
    "We consider three error structures: normal, double exponential\n",
    "and t4, as follows:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "y_i &\\sim \\text{N}(\\mu_i , \\tau^{-1} )\\\\\n",
    "y_i &\\sim \\text{Double exp}( \\mu_i , \\tau^{-1} )\\\\\n",
    "y_i &\\sim \\text{t}_4(\\mu_i , \\tau^{-1} ),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The deviance residuals of the three models have the following forms respectively:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "D_{S_i} &= \\sqrt{\\tau}(y_i-\\mu_i)\\\\\n",
    "D_{S_i} &= sign_i\\sqrt{2\\tau|y_i-\\mu_i|}\\\\\n",
    "D_{S_i} &= sign_i\\sqrt{5\\log\\left( 1 + \\frac{(y_i-\\mu_i)^2}{4} \\right)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 27.5.1 Normal model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pystan as ps\n",
    "import pylab as pl = {'p': 3,\n",
    "'N': 21,\n",
    "'Y': (43, 37, 37, 28, 18, 18, 19, 20, 15, 14, 14, 13, 11, 12, 8, \n",
    "7, 8, 8, 9, 15, 15),\n",
    "'x': np.resize((80, 80, 75, 62, 62, 62, 62, 62, 59, 58, 58, 58, 58, \n",
    "58, 50, 50, 50, 50, 50, 56, 70, 27, 27, 25, 24, 22, 23, 24, 24, \n",
    "23, 18, 18, 17, 18, 19, 18, 18, 19, 19, 20, 20, 20, 89, 88, 90, \n",
    "87, 87, 87, 93, 93, 87, 80, 89, 88, 82, 93, 89, 86, 72, 79, 80, \n",
    "82, 91), (21, 3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "%load_ext stanmagic\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stan -f normal_stack_loss_reg.stan\n",
    "// Linear Model with normal Errors\n",
    "\n",
    "data {\n",
    "  int<lower=0> N;  // number of observations\n",
    "  int<lower=0> p;  // number of explained variables\n",
    "  real Y[N];       // observations\n",
    "  matrix[N,p] x;   // design matrix\n",
    "} \n",
    "\n",
    "// to standardize the x's \n",
    "transformed data {\n",
    "  real z[N,p];\n",
    "  real mean_x[p];\n",
    "  real sd_x[p];\n",
    "  for (j in 1:p) { \n",
    "    mean_x[j] <- mean(col(x,j)); \n",
    "    sd_x[j] <- sd(col(x,j)); \n",
    "    for (i in 1:N)  \n",
    "      z[i,j] <- (x[i,j] - mean_x[j]) / sd_x[j]; \n",
    "  }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real beta0; \n",
    "  real beta[p]; \n",
    "  real<lower=0> sigma; \n",
    "} \n",
    "\n",
    "transformed parameters {\n",
    "  real<lower=0> tau;\n",
    "  real mu[N];\n",
    "  tau <-1/sigma;\n",
    "  for (n in 1:N)\n",
    "    mu[n] <- beta0 + beta[1] * z[n, 1] + beta[2] * z[n, 2] + beta[3] * z[n, 3];\n",
    "}\n",
    "\n",
    "model {\n",
    "  beta0 ~ normal(0, 5); \n",
    "  beta ~ normal(0, 5);\n",
    "  sigma ~ cauchy(0, 2);\n",
    "  for (n in 1:N) \n",
    "    //Y[n] ~ double_exponential(mu[n], sigma); //try you\n",
    "    //Y[n] ~ student_t(4, mu[n], sigma); //try you\n",
    "    Y[n] ~ normal(mu[n], sigma); \n",
    "} \n",
    "\n",
    "generated quantities {\n",
    "  real b0;\n",
    "  real b[p];\n",
    "  real residual[N]; // Pearson residual\n",
    "  real outlier[N];  // test for outlier\n",
    "  real resi_dev[N]; // deviance \n",
    "  real y_rep[N];\n",
    "  real pval:\n",
    "\n",
    "  for (j in 1:p)\n",
    "    b[j] <- beta[j] / sd_x[j];\n",
    "  b0 <- beta0 - b[1] * mean_x[1] - b[2] * mean_x[2] - b[3] * mean_x[3];\n",
    "\n",
    "  for (i in 1:N){\n",
    "    residual[i] = (Y[i] - mu[i]) / sigma;\n",
    "    outlier[i] = step(fabs((Y[i] - mu[i]) / sigma) - 2.5);\n",
    "    resi_dev[i] = \\sqrt(tau)*(Y[i] - mu[i]);\n",
    "    y_rep[i] = normal_rng(mu[i],sigma);\n",
    " }\n",
    " \n",
    "    pval = step(sum(normal_lpdf(Y,mu,sigma))-sum(normal_lpdf(y_rep,mu,sigma)));\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg = pystan.StanModel(file=_stan_model.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg_sample = normal_stack_loss_reg.sampling(data=stacks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = normal_stack_loss_reg_sample.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Some other models for the data to explore.</h4>\n",
    "\n",
    "Please review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stacks_lasso = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> p;\n",
    "  real Y[N];\n",
    "  matrix[N,p] x;\n",
    "} \n",
    "\n",
    "// to standardize the x's \n",
    "transformed data {\n",
    "  real z[N,p];\n",
    "  real mean_x[p];\n",
    "  real sd_x[p];\n",
    "  for (j in 1:p) { \n",
    "    mean_x[j] <- mean(col(x,j)); \n",
    "    sd_x[j] <- sd(col(x,j)); \n",
    "    for (i in 1:N)  \n",
    "      z[i,j] <- (x[i,j] - mean_x[j]) / sd_x[j]; \n",
    "  }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real beta0; \n",
    "  real beta[p]; \n",
    "  real<lower=0> sigmasq; \n",
    "  real<lower=0> phi;\n",
    "} \n",
    "\n",
    "transformed parameters {\n",
    "  real<lower=0> sigma;\n",
    "  real mu[N];\n",
    "\n",
    "  sigma <- sqrt(2) * sigmasq;\n",
    "  for (n in 1:N)\n",
    "    mu[n] <- beta0 + beta[1] * z[n, 1] + beta[2] * z[n, 2] + beta[3] * z[n, 3];\n",
    "}\n",
    "\n",
    "model {\n",
    "  beta0 ~ normal(0, 316); \n",
    "  phi ~ gamma(0.01, 0.01);\n",
    "  beta ~ normal(0, sqrt(phi));\n",
    "  sigmasq ~ inv_gamma(.001, .001); \n",
    "  for (n in 1:N) \n",
    "    Y[n] ~ double_exponential(mu[n], sigmasq); \n",
    "} \n",
    "\n",
    "generated quantities {\n",
    "  real b0;\n",
    "  real b[p];\n",
    "  real outlier_1;\n",
    "  real outlier_3;\n",
    "  real outlier_4;\n",
    "  real outlier_21;\n",
    "\n",
    "  for (j in 1:p)\n",
    "    b[j] <- beta[j] / sd_x[j];\n",
    "  b0 <- beta0 - b[1] * mean_x[1] - b[2] * mean_x[2] - b[3] * mean_x[3];\n",
    "\n",
    "  outlier_1  <- step(fabs((Y[1] - mu[1]) / sigma) - 2.5);\n",
    "  outlier_3  <- step(fabs((Y[3] - mu[3]) / sigma) - 2.5);\n",
    "  outlier_4  <- step(fabs((Y[4] - mu[4]) / sigma) - 2.5);\n",
    "  outlier_21 <- step(fabs((Y[21] - mu[21]) / sigma) - 2.5);\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stacks_init = {'beta0': 10,\n",
    "'beta': [0, 0, 0],\n",
    "'sigmasq': 10,\n",
    "'phi': 0.316}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fit_lasso = ps.stan(model_code=stacks_lasso, data=stacks_data, \n",
    "                  iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "traces = fit_lasso.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "betas = pd.DataFrame(traces['beta'], columns=['beta1','beta2','beta3']).stack().reset_index()\n",
    "betas.columns = 'obs', 'parameter', 'value'\n",
    "g = sb.FacetGrid(betas, col='parameter')\n",
    "g.map(pl.hist, 'value', color=\"steelblue\",lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stacks_ridge = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> p;\n",
    "  real Y[N];\n",
    "  matrix[N,p] x;\n",
    "} \n",
    "\n",
    "// to standardize the x's \n",
    "transformed data {\n",
    "  matrix[N,p] z;\n",
    "  row_vector[p] mean_x;\n",
    "  real sd_x[p];\n",
    "  real d;\n",
    "\n",
    "  for (j in 1:p) { \n",
    "    mean_x[j] <- mean(col(x,j)); \n",
    "    sd_x[j] <- sd(col(x,j)); \n",
    "    for (i in 1:N)  \n",
    "      z[i,j] <- (x[i,j] - mean_x[j]) / sd_x[j]; \n",
    "  }\n",
    "  d <- 4; // degrees of freedom for t\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real beta0; \n",
    "  vector[p] beta;\n",
    "  real<lower=0> sigmasq; \n",
    "  real<lower=0> phi;\n",
    "} \n",
    "\n",
    "transformed parameters {\n",
    "  real<lower=0> sigma;\n",
    "  vector[N] mu;\n",
    "\n",
    "  sigma <- sqrt(d * sigmasq / (d-2)); // t errors on d degrees of freedom\n",
    "  mu <- beta0 + z * beta; \n",
    "}\n",
    "\n",
    "model {\n",
    "  beta0 ~ normal(0, 316); \n",
    "  phi ~ gamma(0.01, 0.01);\n",
    "  beta ~ normal(0, sqrt(phi));\n",
    "  sigmasq ~ inv_gamma(.001, .001); \n",
    "  Y ~ student_t(d, mu, sqrt(sigmasq));\n",
    "} \n",
    "\n",
    "generated quantities {\n",
    "  real b0;\n",
    "  vector[p] b;\n",
    "  real outlier_1;\n",
    "  real outlier_3;\n",
    "  real outlier_4;\n",
    "  real outlier_21;\n",
    "\n",
    "  for (j in 1:p)\n",
    "    b[j] <- beta[j] / sd_x[j];\n",
    "  b0 <- beta0 - mean_x * b;\n",
    "\n",
    "  outlier_1  <- step(fabs((Y[3] - mu[3]) / sigma) - 2.5);\n",
    "  outlier_3  <- step(fabs((Y[3] - mu[3]) / sigma) - 2.5);\n",
    "  outlier_4  <- step(fabs((Y[4] - mu[4]) / sigma) - 2.5);\n",
    "  outlier_21 <- step(fabs((Y[21] - mu[21]) / sigma) - 2.5);\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fit_ridge = ps.stan(model_code=stacks_ridge, data=stacks_data, init=stacks_init,\n",
    "                  iter=10000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ridge_traces = fit_ridge.extract(permuted=True)\n",
    "\n",
    "ridge_betas = pd.DataFrame(ridge_traces['beta'], columns=['beta1','beta2','beta3']).stack().reset_index()\n",
    "ridge_betas.columns = 'obs', 'parameter', 'value'\n",
    "g = sb.FacetGrid(ridge_betas, col='parameter')\n",
    "g.map(pl.hist, 'value', color=\"steelblue\",lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pl.plot(ridge_traces['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "basket_code = '''\n",
    "data {\n",
    "    int<lower=0> K;\n",
    "    int<lower=0> y[K];\n",
    "    int<lower=0> N[K];\n",
    "}\n",
    "parameters {\n",
    "    real theta[K];\n",
    "    real mu;\n",
    "    real<lower=0> s2;\n",
    "}\n",
    "transformed parameters {\n",
    "    real<lower=0, upper=1> p[K];\n",
    "    for (i in 1:K)\n",
    "        p[i] <- inv_logit(theta[i])\n",
    "}\n",
    "model {\n",
    "    mu ~ normal(-1.34, 100)\n",
    "    s2 ~ inv_gamma(0.00005, 0.000005)\n",
    "    for (i in 1:K)\n",
    "        theta[i] ~ normal(mu, sqrt(s2))\n",
    "        y[i] ~ binomial(n[i], p[i])\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "basket_indep_code = '''\n",
    "data {\n",
    "    int<lower=0> K;\n",
    "    int<lower=0> y[K];\n",
    "    int<lower=0> N[K];\n",
    "}\n",
    "parameters {\n",
    "    real theta[K];\n",
    "}\n",
    "transformed parameters {\n",
    "    real<lower=0, upper=1> p[K];\n",
    "    for (i in 1:K)\n",
    "        p[i] <- inv_logit(theta[i])\n",
    "}\n",
    "model {\n",
    "    for (i in 1:K)\n",
    "        theta[i] ~ normal(-1.34, 100)\n",
    "        y[i] ~ binomial(n[i], p[i])\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Bayesian Model selection</h2>\n",
    "\n",
    "The model selection problem is a trade-off between a simple model and good fitting.\n",
    "\n",
    "Ideally, we want to choose the simplest model with best fitting. \n",
    "\n",
    "However good fitting models tend to be more complicated while simpler models tend to be underfit.\n",
    "\n",
    "The model selection methods used in frequentist statistics are typically cross-validation\n",
    "and information criteria, which are the modified residual sum of squares with respect\n",
    "to the model complexity and overfitting.\n",
    "\n",
    "\n",
    "Cross-validation measures the fit of a model on the testing data set, which is not\n",
    "used to fit the model, while the information criteria adjust the measure of fit on the\n",
    "training data set by adding a penalty for model complexity.\n",
    "\n",
    "\n",
    "<h3>The Predictive Accuracy of a Model</h3>\n",
    "\n",
    "In the Bayesian framework, the fit of a model is sometimes called the predictive\n",
    "accuracy of a model (Gelman et al. 2014). We measure the predictive accuracy of\n",
    "a model to a data set $y'$ by log point wise predictive density (**lppd**), calculated as\n",
    "follows:\n",
    "\n",
    "$$\n",
    "lppd:= \\log \\prod_{i=1}^{n'} \\mathbb{E}_{\\theta|y} p(y'|\\theta) = \\sum_{i=1}^{n'} \\log (\\mathbb{E}_{\\theta|y} p(y'|\\theta)) = \\sum_{i=1}^{n'} \\log \\left( \\int p(y'|\\theta) p(\\theta|y)d\\theta \\right).\n",
    "$$\n",
    "\n",
    "Ideally, $y'$ should not be used to fit the model. If we choose $y' = y$, we get the\n",
    "within-sample lppd (denoted by $lppd_{train}$), which is typically larger than the out-ofsample\n",
    "lppd (denoted by $lppd_{test}$). \n",
    "\n",
    "\n",
    "To compute **lppd** in practice, we can evaluate the expectation using draws from the posterior distribution $p(θ|y)$, which we label as $\\theta^{(t)}, t = 1, \\ldots, T$ . The computed lppd is defined as follows:\n",
    " \n",
    "\n",
    "$$\n",
    "\\text{computed } lppd:= \\sum_{i=1}^{n} \\log\\left( \\frac{1}{T} \\sum_{t=1}^{T} p(y'|\\theta^{(t)})\\right).\n",
    "$$\n",
    "\n",
    "<h3>Cross Validation</h3>\n",
    "\n",
    "In Bayesian cross-validation, the data are repeatedly partitioned into a training set\n",
    "$y_{train}$ and a testing set $y_{test}$. \n",
    "\n",
    "\n",
    "For simplicity, we restrict our attention to leave-one-out cross-validation (LOO-CV), where ytest only contains a data point. The Bayesian LOO-CV estimate of out-of-sample lppd is defined as follows:\n",
    "\n",
    "$$\n",
    "lppd_{loo-cv} := \\sum_{i=1}^{n} \\log \\left( p (y_i |\\theta) p (\\theta|y_{−i} ) d\\theta\\right).\n",
    "$$\n",
    "\n",
    "where $y_{−i}$ is the data set without the $i$-th point. The $lppd_{loo-cv}$ can be computed as\n",
    "\n",
    "$$\n",
    "\\text{computed } lppd_{loo-cv} = \\sum_{i=1}^{n} \\log  \\left( \\sum_{t=1}^{T} p(y'|\\theta^{(it)}) \\right),\n",
    "$$\n",
    "\n",
    "where $\\theta^{(it)}, t = 1, \\ldots , T$ are the simulations from the posterior distribution $p(\\theta|y_{-i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deviance Information Criterion DIC</h3>\n",
    "\n",
    "Before describing the DIC, we review another two information criteria employed in\n",
    "frequentist statistics. The Akaike information criterion (AIC) by Akaike (1973) is\n",
    "defined as\n",
    "\n",
    "$$\n",
    "AIC:= − 2\\sum_{i=1}^{n} \\log p (y_i |\\theta_{MLE}) + 2p.\n",
    "$$\n",
    "\n",
    "\n",
    "The Bayesian information criterion (BIC) by Schwarz (1978) is defined as\n",
    "\n",
    "\n",
    "$$\n",
    "AIC:= − 2\\sum_{i=1}^{n} \\log p (y_i |\\theta_{MLE}) + p\\log n\n",
    "$$,\n",
    "\n",
    "where $p$ is the number of parameters. The first common term $− 2\\sum_{i=1}^{n} \\log p (y_i |\\theta_{MLE})$\n",
    "measures the discrepancy between the fitted model and the data. The second term measures the model complexity.\n",
    "\n",
    "\n",
    "<h4> DIC </h4>\n",
    "\n",
    "In the Bayesian framework, we define a similar quantity to measure the discrepancy,\n",
    "$-2 \\sum_{i=1}^{n} \\log p(y_i | \\hat{\\theta})$, where $\\hat{\\theta}$ is the posterior mean. Spiegelhalter et al. (2002) proposed a measure of number of effective parameters, which is defined as the difference\n",
    "between the posterior mean of the deviance and the deviance at the posterior means,\n",
    "as follows:\n",
    "\n",
    "$$\n",
    "p_D := \\widehat{D (\\theta)} - D(\\hat{\\theta}) = −2\\mathbb{E}_{\\theta|y} \\left(\\sum_{i=1}^{n} \\log p (y_i |\\theta)\\right) + 2 \\sum_{i=1}^{n} \\log p(y_i|\\hat{\\theta}),\n",
    "$$)\n",
    "\n",
    "where $D$ is the deviance defined above.\n",
    "\n",
    "\n",
    "Furthermore, they proposed a deviance information criterion (DIC), defined as\n",
    "the deviance at the posterior means plus twice the effective number of parameters,\n",
    "to give\n",
    "\n",
    "$$\n",
    "DIC := D( \\hat{\\theta}) + 2p_D.\n",
    "$$\n",
    "\n",
    "DIC is viewed as a Bayesian analogue of AIC. We prefer the model with smaller\n",
    "DIC. \n",
    "Note that DIC and $p_D$ are sensitive to the level of a hierarchical model. They are appropriate when we are interested in the parameters directly related to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Watanabe-Akaike or widely available information criterion (WAIC)</h3>\n",
    "\n",
    "Watanabe (2010) proposed another measure of number of effective parameters as follows:\n",
    "\n",
    "$$\n",
    "p_{WAIC} := \\widehat{D(\\theta)} + 2lppd_{train} = −2\\mathbb{E}_{\\theta|y} \\left(\\sum_{i=1}^{n} \\log p (y_i |\\theta) \\right) + 2 \\sum_{i=1} ^{n} \\log \\left( \\mathbb{E}_{\\theta|y} p (y_i |\\theta)\\right),\n",
    "$$\n",
    "\n",
    "where $−2lppd_{train}$ plays a role as $D(\\hat{\\theta})$ as in $p_D$. As with AIC and DIC, the Watanabe-\n",
    "Akaike information criterion (WAIC) is defined as\n",
    "\n",
    "$$\n",
    "WAIC:= − 2lppd_{train} + 2p_{WAIC}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Leave-One-Out Information Criterion (LOOIC)</h3>\n",
    "\n",
    "Different from the definition of number of effective parameters in AIC, DIC, and\n",
    "WAIC, we define\n",
    "\n",
    "$$\n",
    "p_{loo} := lppd_{train} − lppd_{loo-cv},\n",
    "$$\n",
    "\n",
    "where $lppd_{loo-cv}$ comes from above. The leave-one-out information criterion (LOOIC) is defined as\n",
    "\n",
    "$$\n",
    "LOOIC:= − 2lppd_{train} + 2p_{loo} = −2lppd_{loo-cv},\n",
    "$$\n",
    "\n",
    "which is reasonable since $lppd_{loo-cv}$ already penalizes the overfitting (or equivalently\n",
    "the model complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
