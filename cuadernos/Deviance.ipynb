{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviance y residuales deviance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Montenegro, [Curso de Estadística Bayesiana](https://github.com/AprendizajeProfundo/Estadistica-Bayesiana), 2020\n",
    "2. Guangyuan Gao, \"Bayesian Claims Reserving Methods in Non-life Insurance with Stan. An Introduction\", Springer, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "En esta lección revisamos la deviance que es una de las estadísticas más importantes en el proceso de evaluación de los modelos estadísticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supóngase que $\\boldsymbol{\\theta}$ es el parámetro de interés en un problema estadístico y que $y_i$ son observaciones independientes con densidad condicional $f_i(\\boldsymbol{\\theta})$. Nótese que en un modelo de regresión por lo general se tiene que $y_i \\sim f(\\boldsymbol{\\theta}, \\mathbf{x}_i)$, en donde $\\mathbf{x}_i$ es el vector de variables predictoras para $y_i$.\n",
    "\n",
    "\n",
    "La log-verosimilitud es una estadística dada por\n",
    "\n",
    "$$\n",
    "\\mathcal{l}(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\ln f_i(y_i|\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "Esta estadística determina la calidad de un modelo, o mejor ajuste de un modelo a los datos, debido a que mayores valores de $\\mathcal{l}(\\boldsymbol{\\theta})$ implica en general mayores valores de las $f_i(y_i|\\boldsymbol{\\theta})$, lo determina una mayor probabilidad a las observaciones. \n",
    "\n",
    "En consecuencia la log-verosimiltud es una medida de ajuste del modelo propuesto a los datos. Mayor log-verosimilitud de un modelo implica mejor ajuste a los datos. Es importante señalar sin embargo que un modelo con mayor log-verosimilitud sea un mejor modelo para el problema. Esto se debe a que en la medida que el número de parámetro crece, se espera en genral un mejor ajuste, en condiciones adecuadas de modelamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deviance se define de manera general por\n",
    "\n",
    "$$\n",
    "D(\\boldsymbol{\\theta})= -2\\sum_{i=1}^N \\ln f_i(y_i|\\boldsymbol{\\theta}) + \\log h(y_1,\\ldots,y_n),\n",
    "$$\n",
    "\n",
    "en donde $h(y_1,\\ldots,y_n)$ es una función de normalización adecuada que depende únicamente de las observaciones. En lo que sigue supondremos que $h(y_1,\\ldots,y_n)=  \\left[\\prod_{i=1}^{n}f(y_i)\\right]$  y que $f(y_i)=1$.Es decir, asumimos que las observaciones tiene la misma densidad marginal. Así adoptaremos la siguiente definición para el resto del curso.\n",
    "\n",
    "$$\n",
    "D(\\boldsymbol{\\theta})= -2\\sum_{i=1}^N \\ln f_i(y_i|\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "Dado que $D(\\boldsymbol{\\theta}) = -2l(\\boldsymbol{\\theta})  $, la deviance es una medida de desajuste, es decir, $D(\\boldsymbol{\\theta})$ es una medida de discrepancia entre el modelo y los datos.\n",
    "\n",
    "Adicionalmente, la log-verosimilitud para una observación es dada por $l_i(\\boldsymbol{\\theta}) = \\log f(y_i|\\boldsymbol{\\theta})$. En consecuencia $l_i(\\boldsymbol{\\theta})$ es una medida de que tan bien es ajustada la observación $y_i$ por el modelo.\n",
    "\n",
    "En el mismo sentido, la deviance de una observación es dada por $D_i(\\boldsymbol{\\theta})= - 2\\log f(y_i|\\boldsymbol{\\theta})$. En consecuencia $D_i(\\boldsymbol{\\theta})$ es una medida de discrepancia  del modelo para la observación $y_i$.\n",
    "\n",
    "De lo anterior se desprende que $D_i(\\boldsymbol{\\theta})$ puede  ser base  para construir estadísticas de error o discrepacia del modelo en relación con la observación $y_i$.\n",
    "\n",
    "## Nota\n",
    "\n",
    "La utilización de la constante 2 en la definición de la deviance es de pura conveniencia histórica, aunque técnicamente no se requiere. Esto está relacionado con aproximaciones asintóticas de la distribución de algunas versiones de $D(\\boldsymbol{\\theta})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$deviance = -2\\sum_{i=1}^n \\ln f(y_i|\\hat{\\theta})$\n",
    "\n",
    "## Caso Bayesiano\n",
    "\n",
    "$lp = \\sum_{i=1}^n \\ln f(y_i|\\hat{\\theta}^{(m)})$ omitiendo constantes\n",
    "\n",
    "$dev^{(m)} = -2 \\sum_{i=1}^n \\ln f(y_i|\\hat{\\theta}^{(m)})$ omitiendo constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuales Deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo saturado es un modelo en el cual se tiene el máximo posible de parámetros. Es bastante común, que el modelo saturado sea aquel en el cual $\\mu_i =y_i$ para cada una e las observaciones. Es aconsejable verificar en los casos específicos cúal es el correpondiente modelo saturado.\n",
    "\n",
    "En esta lección asumerimos que el modelo saturado es dado cuando $\\mu_i =y_i$. Denotaremos por $\\tilde{\\theta}_i$ a cada uno de los parámetros del modelo saturado.  \n",
    "\n",
    "\n",
    "La deviance residual se define por\n",
    "\n",
    "$$\n",
    "D_S(\\boldsymbol{\\theta}) = - 2 \\sum_{i=1}^n \\log f(y_i|\\boldsymbol{\\theta}) + 2 \\sum_{i=1}^n \\log f(y_i|\\boldsymbol{\\tilde{\\theta}_i}).\n",
    "$$\n",
    "\n",
    "\n",
    "La contribución de cada observación a la deviance residual es dada por \n",
    "\n",
    "$$\n",
    "D_{S_i}(\\boldsymbol{\\theta}) = -2 \\log f(y_i|\\boldsymbol{\\theta}) + 2 \\log f(y_i|\\boldsymbol{\\tilde{\\theta}_i}).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para familiarizarlo con el  deviance residual veámos algunos ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuales deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El residual deviance para la observación $i$ es definido por\n",
    "\n",
    "$$\n",
    "dr_i = \\text{sign}_i\\sqrt{D_{S_i}(\\boldsymbol{\\theta})},\n",
    "$$\n",
    "\n",
    "en donde se tiene que $ \\mu_i =E[y_i|\\boldsymbol{\\theta}]$. Adiconalmente  $\\text{sign}_i$ es definida por\n",
    "\n",
    "$$\n",
    "\\text{sign}_i=\\begin{cases} 0, &\\text{ si } (y_i-\\mu_i)<0\\\\\n",
    "1,  &\\text{ en otro caso }.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de Residuales deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Gaussiano homocedástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Se asume que $\\sigma_i = \\sigma$ para todo $i$. En este modelo se tiene que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2\\ln f(y_i|\\boldsymbol{\\theta}) &=  \\frac{(y_i-\\mu_i)^2}{\\sigma} + K,\\\\\n",
    "-2\\ln f(y_i|\\boldsymbol{\\tilde{\\theta}}_i) &=  \\frac{(y_i-y_i)^2}{\\sigma} + K.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que \n",
    "\n",
    "$$\n",
    "dr_i= \\frac{(y_i-\\mu_i)}{\\sigma},\n",
    "$$ \n",
    "\n",
    "coincide con el residual definido en [Residuales Bayesianos](./Residuales_Bayesianos.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo doble exponencial (distribución de Laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso la familia de distribuciones en este caso esa dada por\n",
    "\n",
    "$$\n",
    "f(x\\mid \\mu ,b)={\\frac  {1}{2b}}\\exp \\left(-{\\frac  {|x-\\mu |}{b}}\\right).\n",
    "$$\n",
    "\n",
    "Asumiremos que $b_i=b$ para todo $i$.\n",
    "\n",
    "En este caso se tiene que \n",
    "$$\n",
    "\\begin{align}\n",
    "E[y] &= \\mu\\\\ \n",
    "Var[y] &= 2b^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Se tiene que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2\\ln f(y_i|\\boldsymbol{\\theta}) &=  2\\frac{|y_i-\\mu_i|}{b} + K,\\\\\n",
    "-2\\ln f(y_i|\\boldsymbol{\\tilde{\\theta}}_i) &=  2\\frac{|y_i-y_i|}{b} + K.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que \n",
    "\n",
    "$$\n",
    "dr_i= \\text{sign}_i\\sqrt{\\frac{2|y_i-\\mu_i|}{b}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo $t$-Student\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumimos $\\varphi_i= \\varphi$,  $\\kappa_i = \\kappa$. Entonces\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2\\ln f(y_i|\\boldsymbol{\\theta}) &= (\\kappa+1)\\ln \\left(1+\\frac{1}{\\kappa} \\left(\\frac{y_i-\\mu_i}{\\varphi}\\right)^2\\right) + K\\\\\n",
    "-2\\ln f(y_i|\\boldsymbol{\\tilde{\\theta}}) &= (\\kappa+1)\\ln \\left(1+\\frac{1}{\\kappa} \\left(\\frac{y_i-y_i}{\\varphi}\\right)^2\\right) + K\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que \n",
    "\n",
    "$$\n",
    "dr_i= \\text{sign}_i  \\sqrt{(\\kappa+1)\\ln \\left(1+\\frac{1}{\\kappa} \\left(\\frac{y_i-\\mu_i}{\\varphi}\\right)^2\\right)}.\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones sobre Residuals, Deviance and Deviance Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En el marco bayesiano, podemos generar un conjunto de residuos para una realización de\n",
    "Parámetros posteriores. Por lo tanto, hay cuatro opciones de residuos:\n",
    "\n",
    "1. Elija la media posterior de los parámetros y encuentre un conjunto de residuos.\n",
    "2. Elija aleatoriamente una realización de parámetros y encuentre un conjunto de residuos.\n",
    "3. Obtenga la media posterior de los residuos.\n",
    "4. Obtenga la distribución posterior de los residuos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo. Tres modelos de error para el conjunto de datos stack-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birkes y Dodge (1993) aplican diferentes modelos de regresión a los datos de pérdida de apilamiento muy analizados de Brownlee (1965). Esto presenta 21 respuestas diarias de pérdida de pila $ y $, la cantidad de amoníaco que escapa, con covariables como flujo de aire ($ x_1 $), temperatura ($ x_2 $) y concentración de ácido ($ x_3 $).\n",
    "\n",
    "Primero asumimos una regresión lineal en la esperanza de $ y $, con una variedad de estructuras de error diferentes. Suponemos una regresión lineal en la esperanza de y, es decir,\n",
    "\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}(y_i) = \\mu_i = b_0 + b_1 z_{1i} + b_2 z_{2i} + b_3 z_{3i} \n",
    "$$\n",
    "\n",
    "donde $ z_ {ij} = (x_ {ij} - \\bar {x} _j) / \\text{sd} (x_j) $ son covariables estandarizadas para tener media cero y varianza unitaria. $ b_1, b_2, b_3 $ reciben inicialmente aprioris no informativas independientes.\n",
    "\n",
    "Consideramos tres estructuras de error: normal, doble exponencial y $t_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Normal\n",
    "\n",
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # datos\n",
    "import numpy as np\n",
    "stacks_data = {'p': 3, 'N': 21,\n",
    "'Y': (43, 37, 37, 28, 18, 18, 19, 20, 15, 14, 14, 13, 11, 12, 8, \n",
    "7, 8, 8, 9, 15, 15),\n",
    "'x': np.resize((80, 80, 75, 62, 62, 62, 62, 62, 59, 58, 58, 58, 58, \n",
    "58, 50, 50, 50, 50, 50, 56, 70, 27, 27, 25, 24, 22, 23, 24, 24, \n",
    "23, 18, 18, 17, 18, 19, 18, 18, 19, 19, 20, 20, 20, 89, 88, 90, \n",
    "87, 87, 87, 93, 93, 87, 80, 89, 88, 82, 93, 89, 86, 72, 79, 80, \n",
    "82, 91), (21, 3))}\n",
    "stacks_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#import pystan as ps\n",
    "#import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "%load_ext stanmagic\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stan -f normal_stack_loss_reg.stan\n",
    "// Linear Model with normal Errors\n",
    "\n",
    "data {\n",
    "  int<lower=0> N;  // number of observations\n",
    "  int<lower=0> p;  // number of explained variables\n",
    "  real Y[N];       // observations\n",
    "  matrix[N,p] x;   // design matrix\n",
    "} \n",
    "\n",
    "// to standardize the x's \n",
    "transformed data {\n",
    "  real z[N,p];\n",
    "  real mean_x[p];\n",
    "  real sd_x[p];\n",
    "  for (j in 1:p) { \n",
    "    mean_x[j] <- mean(col(x,j)); \n",
    "    sd_x[j] <- sd(col(x,j)); \n",
    "    for (i in 1:N)  \n",
    "      z[i,j] <- (x[i,j] - mean_x[j]) / sd_x[j]; \n",
    "  }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real beta0; \n",
    "  real beta[p]; \n",
    "  real<lower=0> sigma; \n",
    "} \n",
    "\n",
    "transformed parameters {\n",
    "  real<lower=0> tau;\n",
    "  real mu[N];\n",
    "  tau <-1/sigma;\n",
    "  for (n in 1:N)\n",
    "    mu[n] <- beta0 + beta[1] * z[n, 1] + beta[2] * z[n, 2] + beta[3] * z[n, 3];\n",
    "}\n",
    "\n",
    "model {\n",
    "  beta0 ~ normal(0, 5); \n",
    "  beta ~ normal(0, 5);\n",
    "  sigma ~ cauchy(0, 2);\n",
    "  for (n in 1:N) \n",
    "    //Y[n] ~ double_exponential(mu[n], sigma); //try you\n",
    "    //Y[n] ~ student_t(4, mu[n], sigma); //try you\n",
    "    Y[n] ~ normal(mu[n], sigma); \n",
    "} \n",
    "\n",
    "generated quantities {\n",
    "  real b0;\n",
    "  real b[p];\n",
    "  real residual[N]; // Pearson residual\n",
    "  real resi_dev[N]; // deviance \n",
    "  real y_rep[N];\n",
    " \n",
    "\n",
    "  for (j in 1:p)\n",
    "    b[j] = beta[j] / sd_x[j];\n",
    "      b0 = beta0 - b[1] * mean_x[1] - b[2] * mean_x[2] - b[3] * mean_x[3];\n",
    "\n",
    "  for (i in 1:N){\n",
    "    residual[i] = (Y[i] - mu[i]) / sigma;\n",
    "    // cambiar la siguiente línea en cada caso\n",
    "      resi_dev[i] = (Y[i] - mu[i])/sigma;\n",
    "    // cambiar la siguiente línea en cada caso\n",
    "      y_rep[i] = normal_rng(mu[i],sigma);\n",
    " }\n",
    "    \n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg = pystan.StanModel(file=_stan_model.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg_sample = normal_stack_loss_reg.sampling(data=stacks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_stack_loss_reg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_stack_loss_reg_sample.stansummary([\"y_rep\", \"residual\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_stack_loss_reg_sample.stansummary([\"y_rep\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = normal_stack_loss_reg_sample.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Haga gráficos de las disribuciones de error y error deviance para el caso normal. \n",
    "2. Repita el ejercicio anterior completo (incluya las gráficas que considere) para el caso de las distribuciones Laplace y $t_4$.\n",
    "2. Basado en los residuales trate  de indicar si las observaciones 1, 3,4, 21 son en realidad outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stacks_lasso = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> p;\n",
    "  real Y[N];\n",
    "  matrix[N,p] x;\n",
    "} \n",
    "\n",
    "// to standardize the x's \n",
    "transformed data {\n",
    "  real z[N,p];\n",
    "  real mean_x[p];\n",
    "  real sd_x[p];\n",
    "  for (j in 1:p) { \n",
    "    mean_x[j] <- mean(col(x,j)); \n",
    "    sd_x[j] <- sd(col(x,j)); \n",
    "    for (i in 1:N)  \n",
    "      z[i,j] <- (x[i,j] - mean_x[j]) / sd_x[j]; \n",
    "  }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real beta0; \n",
    "  real beta[p]; \n",
    "  real<lower=0> sigmasq; \n",
    "  real<lower=0> phi;\n",
    "} \n",
    "\n",
    "transformed parameters {\n",
    "  real<lower=0> sigma;\n",
    "  real mu[N];\n",
    "\n",
    "  sigma <- sqrt(2) * sigmasq;\n",
    "  for (n in 1:N)\n",
    "    mu[n] <- beta0 + beta[1] * z[n, 1] + beta[2] * z[n, 2] + beta[3] * z[n, 3];\n",
    "}\n",
    "\n",
    "model {\n",
    "  beta0 ~ normal(0, 316); \n",
    "  phi ~ gamma(0.01, 0.01);\n",
    "  beta ~ normal(0, sqrt(phi));\n",
    "  sigmasq ~ inv_gamma(.001, .001); \n",
    "  for (n in 1:N) \n",
    "    Y[n] ~ double_exponential(mu[n], sigmasq); \n",
    "} \n",
    "\n",
    "generated quantities {\n",
    "  real b0;\n",
    "  real b[p];\n",
    "  real outlier_1;\n",
    "  real outlier_3;\n",
    "  real outlier_4;\n",
    "  real outlier_21;\n",
    "\n",
    "  for (j in 1:p)\n",
    "    b[j] <- beta[j] / sd_x[j];\n",
    "  b0 <- beta0 - b[1] * mean_x[1] - b[2] * mean_x[2] - b[3] * mean_x[3];\n",
    "\n",
    "  outlier_1  <- step(fabs((Y[1] - mu[1]) / sigma) - 2.5);\n",
    "  outlier_3  <- step(fabs((Y[3] - mu[3]) / sigma) - 2.5);\n",
    "  outlier_4  <- step(fabs((Y[4] - mu[4]) / sigma) - 2.5);\n",
    "  outlier_21 <- step(fabs((Y[21] - mu[21]) / sigma) - 2.5);\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stacks_init = {'beta0': 10,\n",
    "'beta': [0, 0, 0],\n",
    "'sigmasq': 10,\n",
    "'phi': 0.316}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fit_ridge = ps.stan(model_code=stacks_ridge, data=stacks_data, init=stacks_init,\n",
    "                  iter=10000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_traces = fit_ridge.extract(permuted=True)\n",
    "\n",
    "ridge_betas = pd.DataFrame(ridge_traces['beta'], columns=['beta1','beta2','beta3']).stack().reset_index()\n",
    "ridge_betas.columns = 'obs', 'parameter', 'value'\n",
    "g = sb.FacetGrid(ridge_betas, col='parameter')\n",
    "g.map(pl.hist, 'value', color=\"steelblue\",lw=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
