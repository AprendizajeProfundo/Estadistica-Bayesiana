{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Curso de Estadística Bayesiana<br> La Estrategia Bayesiana</h1> \n",
    "\n",
    "<h3>Autor</h3>\n",
    "\n",
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "\n",
    "<h3>Fork</h3>\n",
    "\n",
    "<h3>Referencias</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Introducción </h2> \n",
    "\n",
    "Veremos los componentes básicos de un modelo Bayesiano. En esta lección nos basamos en  la tarea propuesta en la lección anterior. Recordemos la narrativa de los datos:\n",
    "\n",
    "Hemos usado un [globo terraqueo a escala.](https://es.wikipedia.org/wiki/Globo_terr%C3%A1queo#/media/Archivo:GlobeSK.jpg)\n",
    "\n",
    "1. El experimento consiste en hacer girar el globo y *pinchar* en algun lugar sin tener ninguna preferencia. El experimento es de tipo dicotómico, debido a que solamente habra dos posibles resultados: agua (1) o tierra (0).\n",
    "\n",
    "- La probabilidad de *pinchar* en agua corresponde al valor verdadero de la proporción de agua y se denotará $\\mu$.\n",
    "- La probabilidad de *pinchar* en tierra corresponde al valor verdadero de la proporción de tierra y es $ 1-\\mu$.\n",
    "- Cada experimento es independiente de los demás."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Componentes del Modelo Bayesiano </h2> \n",
    "\n",
    "<h3> 2.1 Verosimilitud </h3> \n",
    "\n",
    "El primer componente y el más influyente de un modelo Bayesiano es la verosimilitud.\n",
    "La verosimilitud es una fórmula matemática que especifica la plausibilidad de los datos.\n",
    "\n",
    "Lo que esto significa es que la verosimilitud mapea cada conjetura. \n",
    "\n",
    "Puede crear su propia fórmula de verosimilitud a partir de los supuestos básicos de su historia de\n",
    "cómo surgen los datos. Eso es lo que hicimos en el ejemplo anterior. O puede usar\n",
    "Una de las muchas posibilidades que son comunes en las ciencias\n",
    "\n",
    "<h4> Ejemplo 1. Experimento Binomial</h4>\n",
    "\n",
    "Por la forma como hemos descrito la forma de obtención de los datos y si se ha supuesto que el globo se pincharía exáctamente $n$ veces, entonces la probabilidad de obtener $w$ veces agua esta dada por \n",
    "\n",
    "$$P(w| \\mu, N) =  \\frac{n!}{(n-w)!w!} \\mu^w(1-\\mu)^{n-w} = Binomial(w;n,\\mu)$$\n",
    "\n",
    "Esta expresión se puede leer así: El conteo del número de observacions $w$ se distribuye binomialmente, con probabilidad $\\mu$ de obtener agua en cada pinchazo, y se hacen en total $n$ pinchazos.\n",
    "\n",
    "Por ejemplo, si $\\mu= 0.5$, la probabilidad de obtener 6 veces agua en $n=9$ pinchazos es exáctamente\n",
    "0.1640625, como podemos verificar con el siguiente código Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16406250000000006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.binom(n=9,p=0.5).pmf(k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> El papel de la verosimilitud en Estadística </h3>  \n",
    "\n",
    "Notablemente, los supuestos más influyentes tanto en los modelos Bayesianos como en los no Bayesianos son las funciones de verosimilitud y sus relaciones con los parámetros.\n",
    "\n",
    "Los supuestos sobre la versosimilitud influyen en la inferencia para cada pieza de datos. En la medida que el tamaño de la muestra aumenta, la verosimilitud es cada vez más importante. Esto ayuda a explicar por qué las inferencias Bayesianas y las no Bayesianas son a menudo muy similares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2 Los parámetros </h3>\n",
    "\n",
    "Supongamos que $W$ es la variable aleatoria que cuenta el número de veces que se obtiene agua en los $n$ pinchazos. \n",
    "\n",
    "En general las funciones de verosimilitud tienen entradas que pueden ajustarse según el problema. En la verosimilitud binomial del ejemplo, estas entradas son $\\mu$ (la probabilidad de observar $W=w$), $n$ (el tamaño de la muestra) y $w$ (el número de $W$'s).\n",
    "\n",
    "Algunas de estas cantidades son las que deseamos estimar a partir de los datos. Estas cantidades se llamarán **parámetros**. Desde el punto de vista Bayesiano los parámetros y los datos son generados por variables aleatorias.\n",
    "\n",
    "$\\leadsto$ Los parámetros representan *conjeturas o explicaciones* de los datos. \n",
    "\n",
    "\n",
    "En nuestro ejemplo, $n$ y $w$ son conocidas despues de realziar el experimento. Se suponemos que *hemos observado  sin error*,  llegamos a que $\\mu$ es un parámetro desconocido que deseamos estimar.\n",
    "\n",
    "El propósito de nuestra máquina Bayesiana es **describir** lo que nos dicen los datos acerca de $\\mu$.\n",
    "\n",
    "\n",
    "\n",
    "<h4> Sobre la naturaleza de los parámetros </h4>\n",
    "\n",
    "Aunque no es usual en la literatura (en realidad no lo hemos visto antes), vamos a llamar parámetro a la variable aleatoria (o distribución) que genera el parámetro numérico. Ronald Fisher, considerado el pader de la estadística clásica o frecuentista, siempre estuvo en contra del pensamiento Bayesiano. Esto causó que la estadística Bayesiana se mantuviera en el congelador científico por mucho tiempo. Para Fisher, los parámetros no pueden ser variables aleatorias sin cantidades que están ahí y que es necesario estimar. \n",
    "\n",
    "En concepto de los autores de este curso, en realidad es necesario diferenciar el parámetro numérico de la variable aleatoria que lo genera. Esto siempre ha sido confuso en la mayoría de los textos, los cuales utilizan de manera confusa el parámetro y el parámetro numérico. En realidad, podemos estar de acuerdo con Fisher en que el parámetro numérico es una cantidad que está ahi. Simplemente, la teoría Bayesiana indica una de manera científica cómo es que el parámetro llegó ahi. Como la realización de una variable aleatoria. \n",
    "\n",
    "Este orígen del parámetro permite axiomatizar la teoría, es decir, permite construir axiomas a partir  de los cuales es posible construir la teoría estadística moderna. Algo que no es posible con la estadística clásica o frecuentista. Estos axiomas basados en la teoria de la decisión fueron introducidos por Savage en 1954. [Para un introducción simple revise este documento.](https://www.academia.edu/34860240/Teor%C3%ADa_de_la_Decisi%C3%B3n_y_Estad%C3%ADstica_Bayesiana)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.3 Distribución a priori</h3>\n",
    "\n",
    "Para cada parámetro que se desee que calcular, es necesario entregarle a la máquina Bayesiana una distribución a priori que proporciona una *plausabilidad inicial* a cada posible valor del parámetro.  **La apriori  es un conjunto inicial de plausibilidades, para todos los posibles valores del parámtero**.\n",
    "\n",
    "$\\leadsto$ Cuando se tiene una estimación previa del  parámetro, esta estimación se puede convertir en la apriori para los siguientes pasos de la máquina Bayesiana.\n",
    "\n",
    "En la figura 3 se observa  cómo la máquina Bayesiana aprendió una pieza de datos a la vez. Como resultado, cada estimación se convierte en la a priori para el siguiente paso. \n",
    "\n",
    "Este proceso ilustrado en la figura 3, no resuelve el problema de proveerle a la máquina Bayesiana una a priori incial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> ¿De donde provienen las distribuciones a priori? </h4>\n",
    "\n",
    "Desde el punto de vista de la ingeniería, las a priori constituyen un estado inicial requerido para que la máquina Bayesiana pueda empezar a aprender. En la figura 3, se entregó a la máquina una a priori plana.\n",
    "\n",
    "$\\leadsto$ En este caso, la distribución uniforme $\\mathcal{U}(0,1)$, es una distribución que considera que cada valor es igualmente plausible. \n",
    "\n",
    "Las a priori planas son comunes en la práctica. Sinembargo, en general no son la mejor elección, debido a que no aportan ninguna información concreta del parámetro, con la posible excepción de indicar el rango admisible del parámetro. \n",
    "\n",
    "Algunas distribuciones a priori *empujan suavemente* a la máquina Bayesiana. Tales a priori son comunmente llamadas **regularizadoras** o **débilmente informativas**. El uso de este tipo de  a prioris es tan útil que en procedimientos no Bayesianos (de inferencia clásica), se utilizan de forma equivalente (**verosimilitud penalizada**).\n",
    "\n",
    "De manera más general distribuciones a priori se utilizan restringir los valores plausibles de los parámetros a rangos de valores razonables, o para expresar conocimientos a priori de los parámetros, antes que los datos sean observados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Distribuciones a priori razonables para el experimento de esta lección.</h4>\n",
    "\n",
    "Una distribución que resulta muy apropiada para usarse como a priori para la máquina Bayesiana del experimento de esta lección es la distribución Beta, debido a que el rango de valores plausibles (dominio o soporte) de esta distribución es el intervalo $[0,1]$. La función de densidad de probabilidad (fdp) de la distribución Beta con parámetros $\\alpha$ y $\\beta$ es dada por\n",
    "\n",
    "$$ Beta(x;\\alpha,\\beta) = \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{Beta(\\alpha,\\beta)},$$\n",
    "\n",
    "con $x \\in [0,1]$, $\\alpha>0$, $\\beta>0$ y $Beta(\\alpha,\\beta)= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ , en donde $\\Gamma$ es la función Gama. La distribución uniforme $\\mathcal{U}(0,1)$ es el caso particular, en donde $\\alpha=\\beta=1$. Note que en este caso la fdp es dada por $\\mathcal{U}(x;1,0) =1; x \\in [0,1]$.\n",
    "\n",
    "La figura del siguiente [enlace de Wikipedia](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_beta#/media/Archivo:Beta_distribution_pdf.png) se muestra el gráfico de la función de densidad de probabilidad  (fdp) de la distribución Beta para diferente valores de los parámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre la arbitrariedad de las distribuciones a priori\n",
    "\n",
    "La presencia de la distribución a priori en los modelos estadísticos es comunmente criticada por algunas personas. \n",
    "\n",
    "$\\leadsto$ *Algunos afirman que la escogencia de la a priori pued ser tan arbitraria que puede ser utiizada para mentir usando estadística*.\n",
    "\n",
    "Ciertamente, la escogencia es muy flexible. Esto que da al ususario la posibilidad de entregar a la máquina Bayesiana, muy diversas formas de información incial. Usarla para mentir con estadística es por otro lado bastante ingenuo. Evidenciar que se  está manipulando a la máquina para que entregue **resultados manipilados** es realmente muy simple y obvio. Existen formas muy oscuras y sofisticadas para mentir usando estaística.  No es precisamente manipulando la a priori un camino para hacer eso.\n",
    "\n",
    "\n",
    "\n",
    "$\\leadsto$ *Por otro lado, la escogencia de la verosimilitud si bien es más convencional que la escogencia de la a priori, también es subjetiva*. \n",
    "\n",
    "Muchos modelos estadísticos fracasan en describir el mundo grande básicamente por la escogencia de la verosimilitud y no por la a priori. La verosimilitud penalizada es recibida con entusiasmo, porque *evita la escogencia de una a priori*. En realidad, puede mostrarse que en la mayoría de casos, se está haciendo uso de distribuciones a priori y los estimadores penalizados son típicamente Bayesianos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.4 Distribución posterior </h3>\n",
    "\n",
    "Luego de escogida la verosimilitud, los parámetros a se estimados y la distribución a priori asociada a cada parámetro, el modelo Bayesiano es una consecuencia lógica de estas hipótesis y su cosntrucción se basa en el *teorema de Bayes*.\n",
    "\n",
    "Como se ha convenido, en la Estadística Bayesiana se asume que los datos y los parámetros son generados aleatoriamente, con la restricción de que **los parámetros no son observados**. Solamente los datos. \n",
    "\n",
    "Suponamos que $p(w,\\mu)$ representa distribución conjunta de $w$, y $\\mu$. De acuerdo con la reglas multiplicativa de la suma de la probabilidad se tiene que\n",
    "\n",
    "$$p(w,\\mu) = p(w|\\mu)p(\\mu),$$\n",
    "y\n",
    "$$p(w,\\mu) = p(\\mu|w)p(w).$$\n",
    "\n",
    "Por otro lado, las distirbuciones marginales de $\\mu$ y $w$, se obtienen a partir de la rega dela suma como\n",
    "\n",
    "$$p(\\mu) = \\sum_{w} p(\\mu|w)p(w)$$\n",
    "y,\n",
    "\n",
    "$$p(w) = \\int_{w} p(w|\\mu)p(\\mu)d\\mu.$$\n",
    "\n",
    "$P(w)$ a veces llmada la *evidencia* o la *probabilidad de los datos*. En realidad es la **verosimilitud promedio  de los datos con respecto a ala distribución a priori**.\n",
    "\n",
    "\n",
    "Por el teorema de Bayes se tiene que\n",
    "\n",
    "$$p(\\mu|w) = \\frac{ p(w|\\mu)p(\\mu)}{\\int_{w} p(w|\\mu)p(\\mu)d\\mu} \\propto p(w|\\mu)p(\\mu)$$.\n",
    "\n",
    "$\\leadsto$ **<font color=\"red\">Observe que</font>**  $\\int_{w} p(w|\\mu)p(\\mu)d\\mu$ **<font color=\"red\"> no depende de</font>** $\\mu$. Por lo tanto es una constante de normalización para la distribución condicional $p(\\mu|w)$.\n",
    "\n",
    "\n",
    "\n",
    "La distribución definida por la fdp $p(\\mu|w)$ se llama **distribución posterior**. Esta forma de construir la distribución posterior es lo que originó el nombre de **Estadística Bayesiana**. \n",
    "\n",
    "$\\leadsto$ *La distribución posterior combina la información a priori del parámetro, con la versoimilitud de los datos observados, dado el parámetro (no observado)*.\n",
    "\n",
    "Entonces, La distribución posterior contiene la información de la distribución de los valores palusibles del parámetro, teniendo en cuenta la información o creencia a priori y los datos observados. \n",
    "\n",
    "El trabajo de la máquina Bayesiana es determinar a partir de la posterior, cual o cuáles son los valores más probables del prámetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Distribución posterior en el ejemplo de esta lección </h4>\n",
    "\n",
    "Si suponemos que la distribución a priori para nuestro problemas es $Beta(\\alpha,\\beta)$, la posterior es dada por\n",
    "\n",
    "$$p(\\mu|w) \\propto \\mu^w(1-\\mu)^{n-w}  \\times \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} =  \\mu^{w  + \\alpha-1} (1-\\mu)^{n-w\\beta-1},$$\n",
    "\n",
    "es decir que en este caso $p(\\mu|w)= Beta(\\mu;w  + \\alpha, n-w+\\beta)$.\n",
    "\n",
    "Los gráficos de la figura 3 fueron producidos usando esta distribución. En la proxima lección haremos todos los cálculos y gráficos haciendo uso de Python.\n",
    "\n",
    "El siguiente código R, ilustra las distribuciones a priori, versosimilitud y posterior en un ejemplo normal con una observación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a vector of values\n",
    "plot.norm.density<-function(x, mean, sigma,lty){\n",
    " lines(x, dnorm(x,mean,sigma), type=\"l\",lty=lty)\n",
    "}\n",
    "\n",
    " # prior parameters\n",
    "  mu <- 2\n",
    "  tau <- 1\n",
    "\n",
    " # sigma (assumed known)\n",
    "  sigma <- 1\n",
    "  theta <- 3\n",
    "\n",
    "  # observation\n",
    "  x <- 6\n",
    " # posterior parameters\n",
    " mean.p <- (sigma^2*mu + tau^2*x)/(sigma^2 + tau^2)\n",
    " var.p <-   (sigma^2 * tau^2 ) / (sigma^2 + tau^2 )\n",
    "\n",
    " # auxiliar var to plot\n",
    " y<- seq(-2,10, length.out=100)\n",
    " # plot an empty plot\n",
    " \n",
    " plot(y, ylim=c(0,0.6), xlim=c(-2,10), type=\"n\", xlab=expression(theta), ylab=(\"density\"))\n",
    " legend(-2,0.5, legend=c(\"prior\", \"likelihood\", \"posterior\"), lty=1:3,ncol=1)\n",
    "\n",
    " # prior\n",
    "   plot.norm.density(y,mu,tau,lty=1)   #N(2,1)\n",
    " # likelihood\n",
    " plot.norm.density(y,x,sigma,lty=2)    # N(6,1)\n",
    " #posterior\n",
    " plot.norm.density(y,mean.p,sqrt(var.p),lty=3)  # N(4,0.5)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Ejercicio </h4>\n",
    "\n",
    "1. Discuta con sus compañeros el código y pos supuesto el gráfico obtenido-\n",
    "2. Transforme el código a Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Tarea </h2>\n",
    "\n",
    "Use geomaps o una simulación  y realice una estimación mas precisa para el problema de esta lección."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
