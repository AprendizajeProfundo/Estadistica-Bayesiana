{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Curso de Estadística Bayesiana<br> Interpretación de la Media Posterior</h1> \n",
    "\n",
    "<h3>Autor</h3>\n",
    "\n",
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "\n",
    "<h3>Fork</h3>\n",
    "\n",
    "<h3>Referencias</h3>\n",
    "\n",
    "\n",
    "En esta lección vamos a observar en algunos modelos simples, como interpretar la media o esperanza posterior (EAP). Vamos a verificar que la EAP combina la media aprior con la media proveniente de la verosimilitud. Adcionalmente introducimos el  modelo Poisson-Gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. La distribución de Poisson</h2>\n",
    "\n",
    "La distribución de Poisson es el equivalente discreto de la distribución normal. Esta distribución se usa en situaciones de conteo asociadas a la ocurrencia de un evento en unidades del espacio o del tiempo. \n",
    "\n",
    "La distribución tiene un parámetro real positivo, usualmente denotado por $\\lambda$. Este parámetro puede interpretarse como la rata de ocurrencia del evento contabilizado por unidad de espacio o tiempo. Si $N$ es una variable aleatoria con distribución Poisson, su función de probabilidad de masa (fpm) está dada por\n",
    "\n",
    "$$P(N=n|\\lambda) = Poisson(n;\\lambda) = \\frac{\\lambda^n}{n!}e^{-\\lambda n},\\hspace{5mm} n=0,1,2,\\ldots$$\n",
    "\n",
    "La siguiente figura muestra la función de probabilidad de la distribución Poisson, para diferentes valores del parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos de la distribución Poisson\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "lambda_params = [1.7, 5.6, 15.5 ]\n",
    "\n",
    "x = np.arange(poisson.ppf(0.01, np.min(lambda_params)),\n",
    "              poisson.ppf(0.99, np.max(lambda_params)))\n",
    "n_row = 1\n",
    "n_col = 3\n",
    "fig, ax = plt.subplots(n_row, n_col, sharex = True, sharey = True, figsize=(16,4))\n",
    "\n",
    "for i in range(3):\n",
    "    lmbda = lambda_params[i]\n",
    "    y  = poisson.pmf(x, lmbda)\n",
    "    ax[i].vlines(x,0,y,colors='b', lw=10)\n",
    "    ax[i].set_ylim(0,0.35)\n",
    "    ax[i].plot(0,0, label = \"$\\\\lambda$ = {:3.1f}\".format(lmbda), alpha=0 )\n",
    "    ax[i].legend(fontsize=12)\n",
    "    \n",
    "ax[1].set_xlabel('$n$', fontsize = 14)\n",
    "ax[0].set_ylabel('$p(n|\\\\lambda)$',fontsize=14)\n",
    "fig.suptitle('Figura 1. Gráficos de la función de probabilidad de la distribución Poisson',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>2. La distribución Gama</h2>\n",
    "\n",
    "Esta distibución es contínua y tiene soporte en los números reales positivos. Tiene dos parámetros reales postivos: una parámetro de forma $\\alpha$ y un parámetro de escala $\\beta$. La función de densidad de probabilidad (fdp) esta dada por\n",
    "\n",
    "$$f(x|\\lambda,\\beta) = Gama(x;\\alpha,\\beta) = \\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}}x^{\\alpha-1}e^{-x/\\beta}, \\hspace{3mm} \\alpha,\\beta,x >0.$$\n",
    "\n",
    "La figura 2 muestra el gráfico de la función de densidad de la distribución Gama para diferentes valores de los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos de la distribución Gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "\n",
    "alpha_params = [1.0, 2.0, 3.0, 5.0, 9.5, 7.5, 0.5, 1.0, 10.0]\n",
    "beta_params  = [2.0, 2.0, 2.0, 1.0, 0.5, 1.0, 1.0, 5.0, 2.0]\n",
    "\n",
    "\n",
    "idx = np.arange(9)\n",
    "\n",
    "row = np.array([0,0,0,1,1,1,2,2,2])\n",
    "col = np.array([0,1,2,0,1,2,0,1,2])\n",
    "x = np.linspace(0,30,200)\n",
    "              \n",
    "fig, ax = plt.subplots(3, 3,sharex = True, sharey = True, figsize=(12,8))\n",
    "for i in idx:\n",
    "    alpha = alpha_params[i]\n",
    "    beta  = beta_params[i]\n",
    "    y = gamma.pdf(x, alpha, loc=0, scale=beta)\n",
    "    ax[col[i],row[i]].set_ylim(0,0.5)\n",
    "    ax[col[i],row[i]].plot(x,y)\n",
    "    ax[col[i],row[i]].plot(0, 0, label=\"$\\\\alpha$ = {:3.2f}\\n$\\\\beta$ = {:3.2f}\".format(alpha, beta), alpha=0)\n",
    "    ax[col[i],row[i]].legend(fontsize=12)\n",
    "ax[2,1].set_xlabel('$\\\\mu$', fontsize=14)\n",
    "ax[1,0].set_ylabel('$p(\\\\mu)$', fontsize=14)\n",
    "fig.suptitle('Figura 2. Gráficos de la función de densidad de la distribución Gama',fontsize=16)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. El Modelo Poisson - Gama</h2>\n",
    "\n",
    "Por facilidad, en esta sección usaremos la reparametrización $\\beta = 1/\\nu$, para la distribución Gama. En consecuencia $\\nu$ es un parámetro de precisión.  Note que entonces\n",
    "\n",
    "$$f(x|\\lambda,\\nu) = Gama(x;\\alpha,\\nu) = \\frac{\\nu^{\\alpha}}{\\Gamma(\\alpha) }x^{\\alpha-1}e^{-\\nu x}, \\hspace{3mm} \\alpha,\\nu,x >0.$$\n",
    "\n",
    "Dado que el parámetro de la distribución Poisson es un número real positivo, una posible distribución a priori que puede generar tal parámetro es la distribución Gama. En tal caso, la distribución posterior también es una distribución Gama, como se muestra a continuación. Sean $\\mathbf{y}= \\{y_1, \\ldots,y_n\\}$ observaciones provenientes de la distribución $Poisson(\\lambda)$. Entonces la verosimilitud $p( \\mathbf{y}|\\lambda)$ está dada por\n",
    "\n",
    "$$p(\\mathbf{y}|\\lambda) = \\prod_{i=1}^{n} Poisson(y_i;\\lambda) \\propto \\lambda^{\\sum_{i=1}^n y_i} e^{-n\\lambda}$$\n",
    " \n",
    "Por lo tanto la posterior tiene la forma\n",
    "\n",
    "$$p(\\lambda|\\boldsymbol{y}) \\propto  \\left[\\lambda^{\\sum_{i=1}^n y_i} e^{-n\\lambda}\\right] \\lambda^{\\alpha-1}e^{-\\nu \\lambda}  = \\lambda^{\\sum_{i=1}^n y_i +\\alpha -1 } e^{-(n+\\nu)\\lambda},$$\n",
    "\n",
    "Por lo tanto, la posterior es Gama:\n",
    "\n",
    "$$p(\\lambda|\\boldsymbol{y}) = Gama\\left(\\sum_{i=1}^n y_i +\\alpha, n+\\nu\\right)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Media  posterior</h2>\n",
    "\n",
    "La esperanza de la distribución $Gama(\\alpha,\\nu)$, es decir, en la segunda parametrización, es dada por $\\alpha/\\nu$. La media de la observaciones es $\\bar{\\mathbf{y}} = \\tfrac{1}{n}\\sum_i y_i$. Este es la estimacion máximo verosímil de $\\lambda$, es decir, es el valor que maximiza la verosimilitud. En otras palabras, la estimación máximo verosímil de $\\lambda $  es dada por $\\tilde{\\lambda} = \\bar{\\mathbf{y}}$. Por favor verificar.\n",
    "\n",
    "Por otro lado, la media posterior es dada por\n",
    "\n",
    "$$EAP(\\lambda) = E[\\lambda|\\mathbf{y}]=\\hat{\\lambda} = \\frac{\\sum_{i=1}^n y_i + \\alpha}{n + \\nu} = \\frac{n}{n+\\nu} \\bar{\\mathbf{y}} + \\frac{\\nu}{n+\\nu} \\left[ \\frac{\\alpha}{\\nu}\\right].$$\n",
    "\n",
    "Si definimos $\\kappa = \\tfrac{\\nu}{n+\\nu}$, se puede escribir que\n",
    "\n",
    "$$EAP(\\lambda) = E[\\lambda|\\mathbf{y}]=\\hat{\\lambda} =  \\kappa \\left[ \\frac{\\alpha}{\\nu}\\right] + (1-\\kappa) \\bar{\\mathbf{y}} +.$$\n",
    "\n",
    "Que dice que la esperanza posterior es un promedio ponderado entre la media a priori y la estimación máximo veosímil de $\\lambda$.\n",
    "\n",
    "\n",
    "Note que\n",
    "1. $\\hat{\\lambda} \\to \\bar{\\mathbf{y}}$ cuando $\\nu \\to 0$, ($\\kappa \\to 0$).\n",
    "2. $\\hat{\\lambda} \\to \\frac{\\alpha}{\\nu}$ cuando $\\nu \\to \\infty$ , ($\\kappa \\to 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Para discutir en grupo</h4>\n",
    "\n",
    "Discuta con sus compañeros el significado es estos resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Varianza  Posterior</h2>\n",
    "\n",
    "La varianza de la distribución $Gama(\\alpha,\\nu)$, es decir utilizando el ṕarámetro de precisión, es dada por $\\alpha/\\nu^2$ y la varianza posterior es dada por\n",
    "\n",
    "$$Var[\\lambda|\\mathbf{y}] = \\frac{\\sum_{i=1}^n y_i + \\alpha}{(n+\\nu)^2}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Ejercicio. Modelo Binomial - Beta</h2>\n",
    "\n",
    "Encuentre relaciones similares en el modelo Binomial - Beta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Solución</h4>\n",
    "\n",
    "Consideramos la apriori $\\text{Beta}(a,b)$ para $\\mu$. Por lo tanto\n",
    "\n",
    "$$\n",
    "P(\\mu|a,b) \\propto \\mu^{a-1}(1-\\mu)^{b-1}.\n",
    "$$\n",
    "\n",
    "La media a priori es dada por $\\nu = \\tfrac{a}{a+b}$. Sea $M=a+b$. Puede verificarse que $M$ es un parámetro de precisión. Entonces tenemos que $a =\\nu M $ y $b= (1-\\nu)M$. Así, la a priori para $\\mu$ es dada por\n",
    "\n",
    "$$\n",
    "P(\\mu|\\nu,M) \\propto \\mu^{\\nu M}(1-\\mu)^{(1-\\nu)M}.\n",
    "$$\n",
    "\n",
    "\n",
    "La verosimilitud de las observaciones es tal que\n",
    "\n",
    "$$\n",
    "P(Y=\\omega|\\mu,n) \\propto \\mu^{\\omega}(1-\\mu)^{n-\\omega}\n",
    "$$\n",
    "\n",
    "Luego la estimación máximo verosímil de $\\mu$ es $\\tfrac{\\omega}{n}$\n",
    "\n",
    "La densidad posterior es dada por\n",
    "\n",
    "$$\n",
    "p(\\mu|\\omega)\\propto \\mu^{\\omega+\\nu M -1}(1-\\mu)^{n-\\omega + (1-\\nu)M-1},\n",
    "$$\n",
    "\n",
    "es decir que $p(\\mu|\\omega) = \\text{Beta}(\\omega+\\nu M, n-\\omega + (1-\\nu)M)$. Entonces, la esperanza a posterior (EAP) de $\\mu$ es dada por\n",
    "\n",
    "$$\n",
    "EAP(\\mu) = \\frac{n[\\tfrac{\\omega}{n}]+ M[\\nu]}{n+M}\n",
    "$$\n",
    ". \n",
    "Si definimos $\\kappa = \\tfrac{M}{n+M}$, se obtiene que\n",
    "\n",
    "$$\n",
    "EAP(\\mu) = \\kappa [\\nu] + (1-\\kappa) [\\tfrac{\\omega}{n}],\n",
    "$$\n",
    "\n",
    "que de nuevo dice que la esperanza posterior es un promedio ponderado entre la media a priori y la estimación máximo verosímil de $\\mu$.\n",
    "\n",
    "Adicionalmente, la varianza posterior está dada por\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 7. Modelo Gaussiano-Gaussiano</h2>\n",
    "\n",
    "Supongamos que $\\mathbf{y} =(y_1,\\ldots,y_n)'$ es una muestra de una población normal $\\mathcal{N}(\\theta,\\sigma^2)$, en donde $\\sigma^2$ es conocida. Entonces, el parámetro de interés es $\\theta$, el cual es un número real. \n",
    "\n",
    "Es *natural* entonces asumir una distibucion a priori normal para $\\theta$, digamos $\\mathcal{N} (\\mu_0,\\tau)$. En donde vamos a asumir que $\\mu_0$ y $\\tau$ son hiperparámetros dados de antemano.\n",
    "\n",
    "\n",
    "\n",
    "En este caso tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    " \\text{A priori: } &p(\\theta) = \\mathcal{N}(\\mu_0,\\tau^2)\\\\\n",
    " \\text{Verosimilitud: }  &p(\\bar{\\mathbf{y}}|\\theta) = \\mathcal{N}(\\theta,\\sigma^2/n)\\\\\n",
    " \\text{Posterior: } &p(\\theta|\\bar{\\mathbf{y}}) = \\mathcal{N}\\left(\\frac{\\sigma^2 \\mu_0 + n\\tau^2 \\bar{\\mathbf{y}}}{\\sigma^2 + n\\tau^2},\\frac{\\sigma^2\\tau^2}{\\sigma^2+ n\\tau^2}  \\right)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<h3> Ejercicio </h3>\n",
    "\n",
    "Verifique completamente las ecuaciones anteriores.\n",
    "\n",
    "Si definimos $\\kappa= \\tfrac{\\sigma^2}{\\sigma^2+n\\tau^2}$ \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(\\theta|\\bar{\\mathbf{y}}) &= \\kappa\\mu_0 + (1-\\kappa)\\bar{\\mathbf{y}}\\\\\n",
    "Var(\\theta|\\bar{\\mathbf{y}})& =(1-\\kappa)\\sigma^2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "De nuevo, la EAP es un promedio ponderado entre la media a priori y la estimación máximo verosímil del parámetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>8. Discusión</h2>\n",
    "\n",
    "Discuta con sus compañeros las siguientes afirmaciones. Si nos basamos en los resultados de esta lección podemos afirmar que\n",
    "\n",
    "1. En cada caso, la esperanza a posterior es un estimador sesgado que mezcla la información a priori (a partir de la media a priori) y la información que proviene de los datos (estimación máximo verosímil).\n",
    "2. La varianza de la EAP es en general menor que la varianza del estimador máximo verosímil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 9. Tarea</h2>\n",
    "\n",
    "1. Consiga unos datos, para cada uno de los modelos Poisson , Binomial y Normal. Consulte cualquier libro de inferencia estadística. Proponga distribuciones a priori para los parámetros. En el caso normal, asuma que $\\sigma^2$ es conocida.\n",
    "2. Calcule las estimaciones Bayesianas puntuales y de intervalo en cada caso. Presente sus resultados en una tabla.\n",
    "3. Escriba en cada caso la EAP como promedio ponderado de la media a priori y la estimación máximo verosímil en cada caso.\n",
    "4. Haga las gráficas que consider conveniente.\n",
    "5. Repita los puntos 1 a 4, con datos generados por simulación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
