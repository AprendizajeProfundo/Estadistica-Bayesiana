{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#F72585\"><center>Pruebas de hipótesis</center></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección se introducen los conceptos básicos de pruebas de hipótesis desde el punto de vista Bayesiano. \n",
    "\n",
    "Hay diferencias importantes en el enfoque Bayesiano en comparación con el enfoque frecuentista.\n",
    "\n",
    "Desde el punto de vista computacional, el enfoque Bayesiano es basado en el cálculo de integrales, mientras que  el frecuentista por lo general está asociado al cálculo de gradientes. \n",
    "\n",
    "Desde el punto de vista  conceptual, las pruebas frecuentistas en general no son simétricas. Es decir, la hipótesis nula es privilegiada en el sentido que se asume una distribución para una estadística asociada a ella y se usa la evidencia (los datos) para intentar rechazarla. Los roles de hipótesis nula y la  hipótesis alternativa no son intercambiables. \n",
    "\n",
    "En el caso Bayesiano las hipótesis son tratadas como modelos alternativos para un conjunto de datos. Así intercambiar los roles es bastante natural y no altera las conclusiones. Recuerde el lector que en el caso frecuentista, no negar la hipótesis nula no significa realmente aceptar la alternativa. En el caso Bayesiano se pueden invertir los roles, de ser necesario, pues aquí el asunto es determinar cual modelo es más adecuado para explicar la afirmación que se ha puesta a prueba. De hecho, en este caso es incluso posible calcular la probabilidad posterior de cada hipótesis, algo impensable en el caso frecuentista.\n",
    "\n",
    "\n",
    "Adicionalmente, se sabe que en el caso clásico es necesario construir una estadística asociada a la hipótesis nula junto con su respectiva distribución. En el caso Bayesiano el procedimiento es bastante estándar y rutinario. El *factor de Bayes* es siempre la cantidad clave para tomar decisiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Comparación de modelos</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Comparación de dos modelos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponga que se tienen dos modelos paramétricos candidatos, digamos $ M_0 $ y $ M_1 $ para datos $ \\mathbf {y} $ y que los dos modelos tienen vectores de parámetros $ \\boldsymbol {\\theta} _0 $ y $ \\boldsymbol {\\theta} _1 $ respectivamente. Sean $ p_i (\\boldsymbol{\\theta} _i), i = 0,1 $ las respectivas anteriores. Las distribuciones marginales de $ \\mathbf {y} $ están dadas por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(\\mathbf{y}|M_i)= \\int f(\\mathbf{y}| \\boldsymbol{\\theta}_i,M_i)p_i(\\boldsymbol{\\theta}_i)d\\boldsymbol{\\theta}_i\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "De acuerdo con el teorema de Bayes se tiene que \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(M_i|\\mathbf{y})= \\frac{p(\\mathbf{y}|M_i)p(M_i)}{p(\\mathbf{y})}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Y que\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(M_1|\\mathbf{y}) = 1-p(M_0|\\mathbf{y})\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "El enfoque tradicional de la selección del modelo desde la perspectiva Bayesiana se refiere a la siguiente situación.\n",
    "\n",
    "\n",
    "Suponga que los datos observados $ \\mathbf{y} $ son generados por un modelo $ j \\in \\mathfrak{M} $, donde $ \\mathfrak{M} $ es el conjunto finito de modelos competidores.\n",
    "\n",
    "\n",
    "En correspondencia con el modelo $ j $, hay un vector de parámetros desconocido distinto $ \\boldsymbol{\\theta}_j $ de dimensión $ n_j $, y una probabilidad de modelo anterior $ \\pi_j = P ( \\mathfrak {M} = j) $, dónde\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sum_{j\\in \\mathfrak{M}} \\pi_j= 1.\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Supongamos que $ \\boldsymbol {\\Theta}_j $ es el conjunto de todos los valores posibles para $ \\boldsymbol{\\theta} _j $ tal que $ \\boldsymbol {\\theta} _j \\in \\boldsymbol {\\Theta} _j \\subset \\mathcal {R} ^ {n_j} $, y que $ \\boldsymbol {\\Theta} $ sea la colección de todos los $ \\boldsymbol {\\theta} _j $ específicos del modelo.\n",
    "\n",
    "El interés radica en obtener las probabilidades posteriores para los diversos modelos, $ P (\\mathfrak {M} = j | \\boldsymbol {y}) $, ya sea para llegar a un solo \"mejor\" modelo o para determinar la distribución posterior de alguna cantidad de interés $ \\psi $ que es común a todos los modelos a través del promedio de modelos.\n",
    "\n",
    "\n",
    "\n",
    "Debido a las posibles diferencias entre $ \\pi_j $, la elección entre dos modelos (por ejemplo, los modelos 0 y 1) a menudo se basa no en las probabilidades posteriores sino en el factor Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">El factor de Bayes</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos con los datos $ \\mathbf {y} $, que se supone surgieron bajo una de las dos hipótesis $ H_0 $ y $ H_1 $ de acuerdo con una densidad de probabilidad $ pr (\\mathbf {y} | H_0) $ o $ pr ( \\mathbf {y} | H_1) $.\n",
    "\n",
    "\n",
    "\n",
    "Dadas las probabilidades a priori\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "pr(H_0) \\text{ y } pr(H_1) = 1 - pr(H_0),\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Los datos permiten producir probabilidades posteriores dada por \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "pr(H_0| \\mathbf{y} ) \\text{ and } pr(H_1| \\mathbf{y} ) = 1 - pr(H_0| \\mathbf{y} )\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Debido a que cualquier opinión previa se transforma en una opinión posterior a través de la consideración de los datos, la transformación en sí misma representa la evidencia provista por los datos.\n",
    "\n",
    "\n",
    "Para comparar dos modelos, lo usual es la utilización de  la escala *odds* definida por \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "odds= \\frac{probabilidad}{1 - probabilidad} .\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Esta transformación toma una forma simple. Del teorema de Bayes, obtenemos\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Pr[H_k|\\mathbf{y}] = \\frac{Pr[\\mathbf{y}|H_k]P[H_k]}{Pr[\\mathbf{y}|H_0]P[H_0]+Pr[\\mathbf{y}|H_1]P[H_1]}, \\quad k=0,1.\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "De manera que\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\frac{Pr[H_1|\\mathbf{y}] }{Pr[H_0|\\mathbf{y}]}= \\frac{Pr[\\mathbf{y}|H_1]}{Pr[\\mathbf{y}|H_0]} \\frac{P[H_1]}{P[H_0]},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y la transformación se obtiene simplemente multiplicando las probabilidades anteriores por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "B_{10}= \\frac{Pr[\\mathbf{y}|H_1]}{Pr[\\mathbf{y}|H_0]}.\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "La cantidad $B_{10}$ \n",
    "se llama **factor de Bayes**. El factor Bayes mide el peso de la evidencia a favor de $M_1$ against $M_0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En palabras\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\text{odds posterior} = \\text{ factor de Bayes } \\times \\text{odds a priori}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "El factor de Bayes para comparar dos modelos es dado por\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "B_{10} &= \\frac{p(H_1|\\mathbf{y})/p(H_0|\\mathbf{y})}{p(H_1)/p(H_0)}= \\frac{[p(\\mathbf{y}|H_1)p(H_1)]/[p(\\mathbf{y}|H_0)p(H_0)]}{p(H_1)/p(H_0)}\\\\\n",
    "& =\\frac{p(\\mathbf{y}|H_1)}{p(\\mathbf{y}|H_0)} = \\frac{\\text{odds posterior}}{\\text{odds a priori}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "La intuición básica es que la información previa y posterior son combinadas en una proporción que proporciona evidencia a favor de un modelo en contra del otro.\n",
    "\n",
    "\n",
    "Los métodos basados en el factor de Bayes son los métodos dominantes en las pruebas de comparación de modelos en el área Bayesiana. Las pruebas Bayesianas son los análogos Bayesianos de las pruebas de razón de verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Interpretación del Factor de Bayes. Regla de combate</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El factor de Bayes en favor del modelo $M_1$ en contra del modelo $M_0$ es dado por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "B_{10} = \\frac{p(H_1|\\mathbf{y})/p(H_0|\\mathbf{y})}{p(H_1)/p(H_0)},\n",
    "\\end{equation*}\n",
    "$$\n",
    "y la respectiva probabilidad posterior del modelo  1 denotada $\\pi_1 = P(H_1|\\mathbf{y})$ es dada por\n",
    "\n",
    "$$\n",
    "\\pi_1=\\frac{p_1 B_{10}}{p_1 B_{10} + 1- P_1}\n",
    "$$\n",
    "\n",
    "Con esta configuración, si interpretamos el modelo 0 como el modelo nulo, se tiene que el factor de Bayes puede interpretarse de acuerdo con la siguiente regla práctica propuesta por los expertos. Véa las referencias de la lección abajo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Interpretación práctica del factor de Bayes</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{table} Interpretación práctica del factor de Bayes\n",
    "|$B_{10}$|<div style=\"width:65px\">$2\\log B_{10}$ </div>|Evidencia en favor de $M_1$| \n",
    "|---|---|---|\n",
    "|<$\\hspace{1mm}1$ | <$\\hspace{1mm} 0$ |Negativa. Los datos soportan $M_0$|\n",
    "|1 to 3 | 0 to 2           |No importante. Apenas para mencionarlo|\n",
    "|2 to 12| 2 to 5 |Positiva|\n",
    "|12  to 150| 5 to 10|Fuerte|\n",
    "|>$\\hspace{1mm}$150| >$\\hspace{1mm}$ 10 |Muy fuerte|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::{admonition} \n",
    "1. Cuando las hipótesis $ H_1 $ y $ H_0 $ son igualmente probables a priori de modo que $ Pr (H_1) = Pr (H_0) = 0.5 $, el factor Bayes es igual a las probabilidades posteriores a favor de $ H_1 $.\n",
    "2. Sin embargo, las dos hipótesis pueden no ser igualmente probables a priori. En el caso más simple, cuando las dos hipótesis son distribuciones únicas sin parámetros libres (el caso de la prueba \"simple versus simple\"), $ B_ {10} $ es la razón de probabilidad.\n",
    "3. En otros casos, cuando hay parámetros desconocidos bajo una o ambas hipótesis, el factor Bayes todavía está dado por $ B_ {10} $, y, en cierto sentido, sigue teniendo la forma de una razón de probabilidad.\n",
    "4. Las densidades $ Pr (\\mathbf {y} | H_k) $, (k = 0, 1) se obtienen integrando (no maximizando) sobre el espacio del parámetro, de modo que en $ B_ {10} $,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Pr[\\mathbf{y}|H_k] = \\int Pr[H_k|\\boldsymbol{\\theta}_k,\\mathbf{y}]\\pi(\\boldsymbol{\\theta}_k|H_k)d\\boldsymbol{\\theta}_k.\n",
    "\\end{equation*}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Ejemplo 1. Carlin, pag. 54. Preferencias de los consumidores.</span>\n",
    "{cite}`han2001markov`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se consultó a $N=16 $ consumidores sobre su preferencia de un alimento que se conserva en dos sistemas congelados.\n",
    "\n",
    "\n",
    "Sea $Y_i=1$ si el consumidor $i$ seleccionó el alimento refrigerado con el nuevo sistema de congelamiento y $Y_i=0$ en caso contrario.\n",
    "\n",
    "\n",
    "\n",
    "Se asume que las respuestas de los consumidores son independientes.\n",
    "\n",
    "\n",
    "\n",
    "**Modelo observacional**. Definamos $X=\\sum_{i=1}^N Y_i$. Entonces\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "x|\\theta \\sim Bin(N,\\theta)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Por lo que la versosimilitud es dada por   $f(x|\\theta)=\\begin{pmatrix} 16\\\\x \\end{pmatrix} \\theta^x(1-\\theta)^{16-x}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Selección de la apriori**. Para seleccionar lo anterior, observe que la probabilidad como función de $ \\theta $ es proporcional a la densidad beta.\n",
    "\n",
    "\n",
    "\n",
    "Entonces, parece adecuado seleccionar una distribución beta como la anterior para $\\theta$, esto es $p(\\theta) \\varpropto \\theta^{\\alpha -1} (1-\\theta)^{\\beta-1}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **A prior de Jeffreys**: corresponde al caso $\\alpha=\\beta=0.5$\n",
    "2. **A prior plana**: corresponde al caso  $\\alpha=\\beta=1.0$\n",
    "3. **A priori escéptica**:  corresponde al caso $\\alpha=\\beta=2.0$\n",
    "\n",
    "\n",
    "Si $x=13$ es la cantidad de clientes que seleccionan el nuevo sistema, el interés es comparar las hipótesis $H_1: \\theta \\ge 0.6$ and $H_0: \\theta < 0.6$, basado en el pensamiento subjetivo del  usuario de la prueba.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Estimación y probabilidades posteriores</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{table}\n",
    "|A priori|$q_{.025}$|$q_{.500}$ |$q_{.975}$|<div style=\"width:65px\">$p(\\theta>.6 | x))$ </div>  |\n",
    "|---|---|---|---|---|\n",
    "|Jeffreys|0.579 | 0.806| 0.944| 0.9640000|\n",
    "|flat    |0.566 |0.788 |0.932 |0.9540000|\n",
    "|Skeptical|0.544| 0.758| 0.909 |0.9300000|\n",
    "```\n",
    "\n",
    "\n",
    "Usando la distribución a priori se obtiene\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "B_{10}= \\frac{0.954/0.046}{0.4/0.6}= 31.1\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "Finalmente , observemos que la probabilidad de la hipótesis $H_1$ es\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi_1= \\frac{0.4 \\times 31.1}{0.4 \\times 31.1 +1 - 0.4}= 0.954\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Ejemplo 2. Albert, pag. 182. Prueba a una cola para la media de una normal</span>\n",
    "{cite}`lewis1997estimating`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un ejemplo de Berry (1996) citado por Albert, el autor está interesado en determinar su verdadero peso a partir de mediciones diarias en una balanza de baño.\n",
    "\n",
    "\n",
    "Vamos a asumir que las normales son normalmente distribuidas con media $\\mu$ y desviación estándar $\\sigma$.\n",
    "\n",
    "\n",
    "El autor se pesa 10 veces y obtiene las mediciones 182, 172, 173, 176, 176, 180, 173, 174, 179, 175 (libras).\n",
    "\n",
    "\n",
    "\n",
    "Por simplicidad asumimos que $\\sigma=3$ libras. El autor desea testear la hipótesis\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    " H_1: \\mu \\le 175, H_0: \\mu> 175\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "El autor asume poca iniofrmación a priori acerca de su peso verdadero de manera que asigna una apriori para la media dada por $\\mu \\sim N(170,5^2)$\n",
    "\n",
    "\n",
    "El odds  priori de la hipótesis nula $H_0$ es dado por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\frac{p_1}{p_0}= \\frac{P(\\mu \\le 175)}{P(\\mu > 175)} = \\frac{0.8413}{0.1586}=5.3029\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Esto significa que  $H_1$ es cinco veces más probable que la hipótesis alternativa\n",
    "\n",
    "\n",
    "Recordemos que la posterior de $\\mu$ es dada por $N(m,s^2)$, en donde\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "m= \\frac{\\sigma^2 \\times 170 + 10 \\times 5^2 \\times \\bar{y}}{\\sigma^2+10\\times 5^2} = 175.7915 ; \\ s^2= \\frac{\\sigma^2 \\times 5^2 }{\\sigma^2 + 10\\times 5^2}= 0.9320^2\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "luego,\n",
    "\n",
    "\n",
    "\n",
    "$\\frac{\\pi_1}{\\pi_0}= \\frac{P(\\mu \\le 175)}{P( \\mu> 175)} =\\frac{0.1979}{0.8022} =.2467; \\\n",
    "B_{10}= .2467/5.3029 = 0.04652$\n",
    "\n",
    "\n",
    "\n",
    "y $\\pi_1= \\frac{.8413 \\times .04652}{.8413 \\times .04652 + 1 - .8413}= 0.1978$\n",
    "\n",
    "El peso de la evidencia nos conduce a una probabilidad posterior de $H_1$ de 0.1978.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
